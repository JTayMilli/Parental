{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Merging Core CDS-III Data (2007 Wave) ---\n",
      "  - Successfully loaded: DEMOG07.csv (Shape: (1623, 18))\n",
      "  - Successfully loaded: GENMAP07.csv (Shape: (1623, 9))\n",
      "  - Successfully loaded: PCG_CHILD07.csv (Shape: (1608, 616))\n",
      "  - Successfully loaded: CHILD07.csv (Shape: (1506, 557))\n",
      "  - Successfully loaded: ASSESS07.csv (Shape: (1506, 255))\n",
      "  - Successfully loaded: OCG_CHILD07.csv (Shape: (890, 60))\n",
      "  - Successfully loaded: IDMAP07.csv (Shape: (1608, 8))\n",
      "  - Successfully loaded: PCG_HH07.csv (Shape: (1250, 255))\n",
      "Core 2007 CDS merge complete.\n",
      "Step 1 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\01_cds_merged_2007.csv\n",
      "\n",
      "--- Step 2: Merging 2007 PSID Family File ---\n",
      "  - Successfully loaded: FAM2007ER.csv (Shape: (8289, 5240))\n",
      "2007 Family file merge complete.\n",
      "Step 2 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\02_cds_with_family_data_2007.csv\n",
      "\n",
      "--- Step 3: Merging Longitudinal TAS Data ---\n",
      "  - Successfully loaded: TA2005.csv (Shape: (745, 961))\n",
      "  - Successfully loaded: TA2015.csv (Shape: (1641, 1306))\n",
      "TAS merge complete.\n",
      "Step 3 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\03_cds_tas_panel_2007.csv\n",
      "\n",
      "--- Step 4: Processing 2007 Time Diary Data ---\n",
      "  - Successfully loaded: TD_ACTAGG07.csv (Shape: (1442, 1327))\n",
      "  - Successfully loaded: TD_ACT07.csv (Shape: (57813, 35))\n",
      "2007 Time Diary processing complete.\n",
      "Step 4 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\04_time_use_variables_2007.csv\n",
      "\n",
      "--- Step 5: Final Merge ---\n",
      "All 2007 data sources successfully merged.\n",
      "Final dataset saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\final_analysis_dataset_2007.csv\n",
      "Sample dataset saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\sample_final_analysis_dataset_2007.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#   1. Merge all core 2007 CDS files and the Parent Identification File (PID)\n",
    "#      to create a single cross-sectional dataset with permanent IDs.\n",
    "#   2. Merge the 2007 PSID Family File to enrich the 2007 baseline.\n",
    "#   3. Merge the longitudinal Transition to Adulthood (TAS) waves (2005 and 2015).\n",
    "#   4. Process all 2007 Time Diary data to create both aggregate and contextual variables.\n",
    "#   5. Perform the final merge to combine all data sources into a single analysis file.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration: Define all base paths for the 2007 data processing ---\n",
    "BASE_DATA_PATH = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data'\n",
    "CDS_2007_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Child Development Survey', 'CDS2007', '2007')\n",
    "TAS_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Transition into Adulthood Supplement')\n",
    "FAMILY_FILES_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Family Files')\n",
    "# Path for the master Parent Identification File, used to get permanent longitudinal IDs.\n",
    "PID_FILE_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Parent Identification 2023') \n",
    "ANALYSIS_PATH = os.path.join(BASE_DATA_PATH, 'Processed Data 2007')\n",
    "\n",
    "\n",
    "# --- Helper Function to Load Data ---\n",
    "def load_data(file_path, required=True):\n",
    "    \"\"\"Safely loads a CSV file, printing its status and shape.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"  - Successfully loaded: {os.path.basename(file_path)} (Shape: {df.shape})\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        if required:\n",
    "            print(f\"  - FATAL ERROR: Required file not found at {file_path}\")\n",
    "            raise\n",
    "        else:\n",
    "            print(f\"  - Warning: Optional file not found, skipping: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "\n",
    "# --- STEP 1: Merge Core CDS-III Data (2007 Wave) ---\n",
    "def merge_core_cds_data_2007():\n",
    "    \"\"\"\n",
    "    Loads and merges all raw 2007 CDS files. This now includes merging both the PID file and\n",
    "    the original GENMAP file to ensure permanent identifiers are robustly included.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Merging Core CDS-III Data (2007 Wave) ---\")\n",
    "    \n",
    "    demog_df = load_data(os.path.join(CDS_2007_PATH, 'DEMOG07.csv'))\n",
    "    gen_map_df = load_data(os.path.join(CDS_2007_PATH, 'GENMAP07.csv'), required=False)\n",
    "    pcg_chld_df = load_data(os.path.join(CDS_2007_PATH, 'PCG_CHILD07.csv'))\n",
    "    child_df = load_data(os.path.join(CDS_2007_PATH, 'CHILD07.csv'), required=False)\n",
    "    assessmt_df = load_data(os.path.join(CDS_2007_PATH, 'ASSESS07.csv'), required=False)\n",
    "    ocg_chld_df = load_data(os.path.join(CDS_2007_PATH, 'OCG_CHILD07.csv'), required=False)\n",
    "    idmap_df = load_data(os.path.join(CDS_2007_PATH, 'IDMAP07.csv'))\n",
    "    pcg_hhld_df = load_data(os.path.join(CDS_2007_PATH, 'PCG_HH07.csv'))\n",
    "    pid_df = load_data(os.path.join(PID_FILE_PATH, 'PID23.csv'))\n",
    "\n",
    "    # Prepare base dataframe and standardize identifiers\n",
    "    demog_df.rename(columns={'DEMID07': 'ID_2007', 'DEMSN07': 'SN_2007'}, inplace=True)\n",
    "    merged_df = demog_df\n",
    "\n",
    "    # Merge PID file to add permanent IDs. PID16/PID17 correspond to 2007 identifiers.\n",
    "    pid_subset = pid_df[['PID2', 'PID3', 'PID16', 'PID17']].copy()\n",
    "    pid_subset.rename(columns={'PID2': 'ER30001_pid', 'PID3': 'ER30002_pid', 'PID16': 'ID_2007', 'PID17': 'SN_2007'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, pid_subset, on=['ID_2007', 'SN_2007'], how='left')\n",
    "\n",
    "    # Also merge GENMAP to retain original data structure and provide a cross-check.\n",
    "    if gen_map_df is not None:\n",
    "        gen_map_sub = gen_map_df[['GENID07', 'GENSN07', 'CH07_ID', 'CH07_PN']].rename(columns={\n",
    "            'GENID07': 'ID_2007', 'GENSN07': 'SN_2007', \n",
    "            'CH07_ID': 'ER30001', 'CH07_PN': 'ER30002'\n",
    "        })\n",
    "        merged_df = pd.merge(merged_df, gen_map_sub, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    \n",
    "    # Merge all child-level files\n",
    "    child_files = {'pcg_child07': pcg_chld_df, 'child07': child_df, 'assess07': assessmt_df, 'ocg_child07': ocg_chld_df}\n",
    "    key_map = {'pcg_child07': ('PCHID07', 'PCHSN07'), 'child07': ('CHLDID07', 'CHLDSN07'), 'assess07': ('ASMID07', 'ASMSN07'), 'ocg_child07': ('OCHID07', 'OCHSN07')}\n",
    "    \n",
    "    for name, df in child_files.items():\n",
    "        if df is not None:\n",
    "            key_id, key_sn = key_map[name]\n",
    "            df.rename(columns={key_id: 'ID_2007', key_sn: 'SN_2007'}, inplace=True)\n",
    "            merged_df = pd.merge(merged_df, df, on=['ID_2007', 'SN_2007'], how='left', suffixes=('', f'_{name}'))\n",
    "            \n",
    "    # Merge household data via IDMAP\n",
    "    idmap_df.rename(columns={'CHILDID07': 'ID_2007', 'CHILDSN07': 'SN_2007'}, inplace=True)\n",
    "    pcg_hhld_df.rename(columns={'PHHID07': 'PCGID07', 'PHHSN07': 'PCGSN07'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, idmap_df, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    merged_df = pd.merge(merged_df, pcg_hhld_df, on=['PCGID07', 'PCGSN07'], how='left', suffixes=('', '_pcghh07'))\n",
    "    \n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(\"Core 2007 CDS merge complete.\")\n",
    "    return merged_df\n",
    "\n",
    "# --- STEP 2: Merge 2007 PSID Family File ---\n",
    "def merge_family_file_2007(base_df):\n",
    "    \"\"\"\n",
    "    Enriches the core CDS dataset by merging the 2007 PSID Family File.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 2: Merging 2007 PSID Family File ---\")\n",
    "    family_df = load_data(os.path.join(FAMILY_FILES_PATH, 'fam2007er', 'FAM2007ER.csv'))\n",
    "    \n",
    "    family_df.rename(columns={'ER36002': 'ID_2007'}, inplace=True)\n",
    "    \n",
    "    enriched_df = pd.merge(base_df, family_df, on='ID_2007', how='left', suffixes=('', '_fam2007'))\n",
    "    \n",
    "    enriched_df = enriched_df.loc[:, ~enriched_df.columns.duplicated()]\n",
    "    print(\"2007 Family file merge complete.\")\n",
    "    return enriched_df\n",
    "\n",
    "# --- STEP 3: Merge Longitudinal TAS Data ---\n",
    "def merge_longitudinal_tas_data(base_df):\n",
    "    \"\"\"\n",
    "    Merges the Transition to Adulthood (TAS) waves onto the base 2007 CDS dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 3: Merging Longitudinal TAS Data ---\")\n",
    "    \n",
    "    tas_files = {\n",
    "        '05': (os.path.join(TAS_PATH, 'ta2005', 'TA2005.csv'), 'TA050004', 'TA050005'),\n",
    "        '15': (os.path.join(TAS_PATH, 'ta2015', 'TA2015.csv'), 'TA150004', 'TA150005')\n",
    "    }\n",
    "    \n",
    "    longitudinal_df = base_df.copy()\n",
    "    for year, (path, id_col, pn_col) in tas_files.items():\n",
    "        tas_df = load_data(path, required=False)\n",
    "        if tas_df is not None:\n",
    "            tas_df.rename(columns={id_col: 'ER30001', pn_col: 'ER30002'}, inplace=True)\n",
    "            longitudinal_df = pd.merge(longitudinal_df, tas_df, on=['ER30001', 'ER30002'], how='left', suffixes=('', f'_tas{year}'))\n",
    "    \n",
    "    longitudinal_df = longitudinal_df.loc[:, ~longitudinal_df.columns.duplicated()]\n",
    "    print(\"TAS merge complete.\")\n",
    "    return longitudinal_df\n",
    "\n",
    "# --- STEP 4: Process 2007 Time Diary Data ---\n",
    "def process_time_diaries_2007():\n",
    "    \"\"\"\n",
    "    Creates a standalone DataFrame with comprehensive time-use variables for the 2007 wave.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 4: Processing 2007 Time Diary Data ---\")\n",
    "    \n",
    "    td_agg_df = load_data(os.path.join(CDS_2007_PATH, 'TD_ACTAGG07.csv'))\n",
    "    td_activity_df = load_data(os.path.join(CDS_2007_PATH, 'TD_ACT07.csv'))\n",
    "\n",
    "    child_identifiers = td_agg_df[['AGGRID07', 'AGGRSN07']].copy().rename(columns={'AGGRID07': 'ID_2007', 'AGGRSN07': 'SN_2007'})\n",
    "\n",
    "    part_a_df = calculate_aggregate_weekly_hours_2007(child_identifiers.copy(), td_agg_df)\n",
    "    part_b_df = calculate_intensive_parenting_time_2007(td_activity_df)\n",
    "    \n",
    "    time_use_df = pd.merge(part_a_df, part_b_df, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    time_use_df.fillna(0, inplace=True)\n",
    "    print(\"2007 Time Diary processing complete.\")\n",
    "    return time_use_df\n",
    "\n",
    "def calculate_aggregate_weekly_hours_2007(base_df, td_agg_df):\n",
    "    \"\"\"Calculates weekly average hours for 2007.\"\"\"\n",
    "    td_agg_df.rename(columns={'AGGRID07': 'ID_2007', 'AGGRSN07': 'SN_2007'}, inplace=True)\n",
    "    panel_with_agg = pd.merge(base_df, td_agg_df, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    activity_codes = [f'39{i:02d}' for i in range(1, 40)]\n",
    "    for code in activity_codes:\n",
    "        wd_col, we_col = f'WD07{code}', f'WE07{code}'\n",
    "        new_col = f'weekly_avg_hrs_cat_{code}_07'\n",
    "        if wd_col in panel_with_agg.columns and we_col in panel_with_agg.columns:\n",
    "            wd_sec = panel_with_agg[wd_col].fillna(0)\n",
    "            we_sec = panel_with_agg[we_col].fillna(0)\n",
    "            panel_with_agg[new_col] = ((wd_sec * 5) + (we_sec * 2)) / 3600\n",
    "    new_cols = ['ID_2007', 'SN_2007'] + [f'weekly_avg_hrs_cat_{code}_07' for code in activity_codes]\n",
    "    return panel_with_agg[[col for col in new_cols if col in panel_with_agg.columns]]\n",
    "\n",
    "def calculate_intensive_parenting_time_2007(td_activity_df):\n",
    "    \"\"\"Calculates 'intensive parenting' measures for 2007.\"\"\"\n",
    "    skill_codes = [5490, 5491, 5492, 5493, 5494, 8010, 8011, 8012, 5040, 8020, 8030, 8040, 8090, 8510, 8520, 8211, 8212, 8213, 8214, 8215, 8221, 8222, 8223]\n",
    "    skill_df = td_activity_df[td_activity_df['COLA_07'].isin(skill_codes)].copy()\n",
    "    wd_skill_df = skill_df[skill_df['DIARY_07'] == 0]\n",
    "    we_skill_df = skill_df[skill_df['DIARY_07'] == 1]\n",
    "    \n",
    "    child_ids = td_activity_df[['TDID07', 'TDSN07']].drop_duplicates().rename(columns={'TDID07': 'ID_2007', 'TDSN07': 'SN_2007'})\n",
    "\n",
    "    # Use 'COLIB_07' for mother and 'COLIC_07' for father participation.\n",
    "    for day_type, df in [('wd', wd_skill_df), ('we', we_skill_df)]:\n",
    "        for parent, col in [('mother', 'COLIB_07'), ('father', 'COLIC_07')]:\n",
    "            mask = df[col] == 1\n",
    "            time = df[mask].groupby(['TDID07', 'TDSN07'])['DUR_07'].sum().reset_index()\n",
    "            time.rename(columns={'DUR_07': f'{parent}_interactive_{day_type}_sec_07', 'TDID07': 'ID_2007', 'TDSN07': 'SN_2007'}, inplace=True)\n",
    "            child_ids = pd.merge(child_ids, time, on=['ID_2007', 'SN_2007'], how='left')\n",
    "\n",
    "    cols_to_fill = [f'{p}_interactive_{d}_sec_07' for p in ['mother', 'father'] for d in ['wd', 'we']]\n",
    "    for col in cols_to_fill:\n",
    "        if col not in child_ids.columns: child_ids[col] = 0\n",
    "        else: child_ids[col] = child_ids[col].fillna(0)\n",
    "            \n",
    "    child_ids['parent_interactive_skill_hrs_wk_07'] = \\\n",
    "        (((child_ids['mother_interactive_wd_sec_07'] + child_ids['father_interactive_wd_sec_07']) * 5) +\n",
    "         ((child_ids['mother_interactive_we_sec_07'] + child_ids['father_interactive_we_sec_07']) * 2)) / 3600\n",
    "         \n",
    "    return child_ids[['ID_2007', 'SN_2007', 'parent_interactive_skill_hrs_wk_07']]\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(ANALYSIS_PATH):\n",
    "        os.makedirs(ANALYSIS_PATH)\n",
    "\n",
    "    # Step 1\n",
    "    core_cds_df = merge_core_cds_data_2007()\n",
    "    path_step1 = os.path.join(ANALYSIS_PATH, '01_cds_merged_2007.csv')\n",
    "    core_cds_df.to_csv(path_step1, index=False)\n",
    "    print(f\"Step 1 intermediate file saved to: {path_step1}\")\n",
    "\n",
    "    # Step 2\n",
    "    cds_family_df = merge_family_file_2007(core_cds_df)\n",
    "    path_step2 = os.path.join(ANALYSIS_PATH, '02_cds_with_family_data_2007.csv')\n",
    "    cds_family_df.to_csv(path_step2, index=False)\n",
    "    print(f\"Step 2 intermediate file saved to: {path_step2}\")\n",
    "\n",
    "    # Step 3\n",
    "    cds_tas_panel = merge_longitudinal_tas_data(cds_family_df)\n",
    "    path_step3 = os.path.join(ANALYSIS_PATH, '03_cds_tas_panel_2007.csv')\n",
    "    cds_tas_panel.to_csv(path_step3, index=False)\n",
    "    print(f\"Step 3 intermediate file saved to: {path_step3}\")\n",
    "\n",
    "    # Step 4\n",
    "    time_use_variables = process_time_diaries_2007()\n",
    "    path_step4 = os.path.join(ANALYSIS_PATH, '04_time_use_variables_2007.csv')\n",
    "    time_use_variables.to_csv(path_step4, index=False)\n",
    "    print(f\"Step 4 intermediate file saved to: {path_step4}\")\n",
    "\n",
    "    # Step 5: Final Merge\n",
    "    print(\"\\n--- Step 5: Final Merge ---\")\n",
    "    final_dataset = pd.merge(cds_tas_panel, time_use_variables, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated()]\n",
    "    print(\"All 2007 data sources successfully merged.\")\n",
    "\n",
    "    # Save final outputs\n",
    "    final_path = os.path.join(ANALYSIS_PATH, 'final_analysis_dataset_2007.csv')\n",
    "    final_dataset.to_csv(final_path, index=False)\n",
    "    print(f\"Final dataset saved to: {final_path}\")\n",
    "    \n",
    "    if len(final_dataset) >= 1000:\n",
    "        sample_df = final_dataset.sample(n=1000, random_state=42)\n",
    "        sample_path = os.path.join(ANALYSIS_PATH, 'sample_final_analysis_dataset_2007.csv')\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        print(f\"Sample dataset saved to: {sample_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
