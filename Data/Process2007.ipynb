{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Merging Core CDS-III Data (2007 Wave) ---\n",
      "  - Successfully loaded: DEMOG07.csv (Shape: (1623, 18))\n",
      "  - Successfully loaded: GENMAP07.csv (Shape: (1623, 9))\n",
      "  - Successfully loaded: PCG_CHILD07.csv (Shape: (1608, 616))\n",
      "  - Successfully loaded: CHILD07.csv (Shape: (1506, 557))\n",
      "  - Successfully loaded: ASSESS07.csv (Shape: (1506, 255))\n",
      "  - Successfully loaded: OCG_CHILD07.csv (Shape: (890, 60))\n",
      "  - Successfully loaded: IDMAP07.csv (Shape: (1608, 8))\n",
      "  - Successfully loaded: PCG_HH07.csv (Shape: (1250, 255))\n",
      "Core 2007 CDS merge complete.\n",
      "Step 1 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\01_cds_merged_2007.csv\n",
      "\n",
      "--- Step 2: Merging 2007 PSID Family File ---\n",
      "  - Successfully loaded: FAM2007ER.csv (Shape: (8289, 5240))\n",
      "2007 Family file merge complete.\n",
      "Step 2 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data 2007\\02_cds_with_family_data_2007.csv\n",
      "\n",
      "--- Step 3: Merging Longitudinal TAS Data ---\n",
      "  - Successfully loaded: TA2005.csv (Shape: (745, 961))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ER30001'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18576\\1103816240.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mcds_family_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_step2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mStep 2 intermediate file saved to: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpath_step2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;31m# Step 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0mcds_tas_panel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_longitudinal_tas_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcds_family_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[0mpath_step3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mANALYSIS_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'03_cds_tas_panel_2007.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mcds_tas_panel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_step3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mStep 3 intermediate file saved to: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpath_step3\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18576\\1103816240.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(base_df)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpn_col\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtas_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mtas_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequired\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtas_df\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mtas_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mid_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ER30001'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpn_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ER30002'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mlongitudinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongitudinal_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtas_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ER30001'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ER30002'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33mf'\u001b[0m\u001b[1;33m_tas\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mlongitudinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlongitudinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mlongitudinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TAS merge complete.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1307\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1312\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ER30001'"
     ]
    }
   ],
   "source": [
    "# build_master_dataset_2007.py\n",
    "#\n",
    "# Purpose:\n",
    "# This script serves as the single, definitive program for constructing the complete\n",
    "# longitudinal analysis dataset from the raw 2007 PSID-CDS files. It is\n",
    "# a direct adaptation of the 2002 master script, updated for the 2007 file names\n",
    "# and variable structures. It saves intermediate files at each major step,\n",
    "# making it ideal for use in a Jupyter Notebook.\n",
    "#\n",
    "# The workflow is as follows:\n",
    "#   1. Merge all core 2007 CDS files into a single cross-sectional dataset.\n",
    "#   2. Merge the 2007 PSID Family File to enrich the 2007 baseline.\n",
    "#   3. Merge the longitudinal Transition to Adulthood (TAS) waves.\n",
    "#   4. Process all 2007 Time Diary data to create both aggregate and contextual variables.\n",
    "#   5. Perform the final merge to combine all data sources.\n",
    "#\n",
    "# Author: Gemini\n",
    "# Date: October 1, 2025\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration: Define all base paths ---\n",
    "BASE_DATA_PATH = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data'\n",
    "CDS_2007_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Child Development Survey', 'CDS2007', '2007')\n",
    "TAS_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Transition into Adulthood Supplement')\n",
    "FAMILY_FILES_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Family Files')\n",
    "ANALYSIS_PATH = os.path.join(BASE_DATA_PATH, 'Processed Data 2007')\n",
    "\n",
    "# --- Helper Function to Load Data ---\n",
    "def load_data(file_path, required=True):\n",
    "    \"\"\"Safely loads a CSV file, printing its status and shape.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"  - Successfully loaded: {os.path.basename(file_path)} (Shape: {df.shape})\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        if required:\n",
    "            print(f\"  - FATAL ERROR: Required file not found at {file_path}\")\n",
    "            raise\n",
    "        else:\n",
    "            print(f\"  - Warning: Optional file not found, skipping: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "\n",
    "# --- STEP 1: Merge Core CDS-III Data (2007 Wave) ---\n",
    "def merge_core_cds_data_2007():\n",
    "    \"\"\"\n",
    "    Loads and merges all raw 2007 CDS files into a single cross-sectional dataset.\n",
    "    This forms the foundational data for the 2007 wave.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Merging Core CDS-III Data (2007 Wave) ---\")\n",
    "    \n",
    "    demog_df = load_data(os.path.join(CDS_2007_PATH, 'DEMOG07.csv'))\n",
    "    gen_map_df = load_data(os.path.join(CDS_2007_PATH, 'GENMAP07.csv'))\n",
    "    pcg_chld_df = load_data(os.path.join(CDS_2007_PATH, 'PCG_CHILD07.csv'))\n",
    "    child_df = load_data(os.path.join(CDS_2007_PATH, 'CHILD07.csv'), required=False)\n",
    "    assessmt_df = load_data(os.path.join(CDS_2007_PATH, 'ASSESS07.csv'), required=False)\n",
    "    ocg_chld_df = load_data(os.path.join(CDS_2007_PATH, 'OCG_CHILD07.csv'), required=False)\n",
    "    idmap_df = load_data(os.path.join(CDS_2007_PATH, 'IDMAP07.csv'))\n",
    "    pcg_hhld_df = load_data(os.path.join(CDS_2007_PATH, 'PCG_HH07.csv'))\n",
    "\n",
    "    demog_df.rename(columns={'DEMID07': 'ID_2007', 'DEMSN07': 'SN_2007'}, inplace=True)\n",
    "    \n",
    "    # Robustly select and rename permanent ID columns to prevent silent failures\n",
    "    gen_map_sub = gen_map_df[['GENID07', 'GENSN07', 'CH_ID68', 'CH_PN']].rename(columns={\n",
    "        'GENID07': 'ID_2007', 'GENSN07': 'SN_2007', \n",
    "        'CH_ID68': 'ER30001', 'CH_PN': 'ER30002'\n",
    "    })\n",
    "    \n",
    "    merged_df = pd.merge(demog_df, gen_map_sub, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    \n",
    "    child_files = {'pcg_child07': pcg_chld_df, 'child07': child_df, 'assess07': assessmt_df, 'ocg_child07': ocg_chld_df}\n",
    "    key_map = {'pcg_child07': ('PCHID07', 'PCHSN07'), 'child07': ('CHLDID07', 'CHLDSN07'), 'assess07': ('ASMID07', 'ASMSN07'), 'ocg_child07': ('OCHID07', 'OCHSN07')}\n",
    "    \n",
    "    for name, df in child_files.items():\n",
    "        if df is not None:\n",
    "            key_id, key_sn = key_map[name]\n",
    "            df.rename(columns={key_id: 'ID_2007', key_sn: 'SN_2007'}, inplace=True)\n",
    "            merged_df = pd.merge(merged_df, df, on=['ID_2007', 'SN_2007'], how='left', suffixes=('', f'_{name}'))\n",
    "            \n",
    "    idmap_df.rename(columns={'CHILDID07': 'ID_2007', 'CHILDSN07': 'SN_2007'}, inplace=True)\n",
    "    pcg_hhld_df.rename(columns={'PHHID07': 'PCGID07', 'PHHSN07': 'PCGSN07'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, idmap_df, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    merged_df = pd.merge(merged_df, pcg_hhld_df, on=['PCGID07', 'PCGSN07'], how='left', suffixes=('', '_pcghh07'))\n",
    "    \n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(\"Core 2007 CDS merge complete.\")\n",
    "    return merged_df\n",
    "\n",
    "# --- STEP 2: Merge 2007 PSID Family File ---\n",
    "def merge_family_file_2007(base_df):\n",
    "    \"\"\"\n",
    "    Enriches the core CDS dataset by merging the 2007 PSID Family File.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 2: Merging 2007 PSID Family File ---\")\n",
    "    family_df = load_data(os.path.join(FAMILY_FILES_PATH, 'fam2007er', 'FAM2007ER.csv'))\n",
    "    \n",
    "    family_df.rename(columns={'ER36002': 'ID_2007'}, inplace=True)\n",
    "    \n",
    "    enriched_df = pd.merge(base_df, family_df, on='ID_2007', how='left', suffixes=('', '_fam2007'))\n",
    "    \n",
    "    enriched_df = enriched_df.loc[:, ~enriched_df.columns.duplicated()]\n",
    "    print(\"2007 Family file merge complete.\")\n",
    "    return enriched_df\n",
    "\n",
    "# --- STEP 3: Merge Longitudinal TAS Data ---\n",
    "def merge_longitudinal_tas_data(base_df):\n",
    "    \"\"\"\n",
    "    Merges the Transition to Adulthood (TAS) waves onto the base 2007 CDS dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 3: Merging Longitudinal TAS Data ---\")\n",
    "    \n",
    "    tas_files = {\n",
    "        '05': (os.path.join(TAS_PATH, 'ta2005', 'TA2005.csv'), 'TA050004', 'TA050005'),\n",
    "        '15': (os.path.join(TAS_PATH, 'ta2015', 'TA2015.csv'), 'TA150004', 'TA150005')\n",
    "    }\n",
    "    \n",
    "    longitudinal_df = base_df.copy()\n",
    "    for year, (path, id_col, pn_col) in tas_files.items():\n",
    "        tas_df = load_data(path, required=False)\n",
    "        if tas_df is not None:\n",
    "            tas_df.rename(columns={id_col: 'ER30001', pn_col: 'ER30002'}, inplace=True)\n",
    "            longitudinal_df = pd.merge(longitudinal_df, tas_df, on=['ER30001', 'ER30002'], how='left', suffixes=('', f'_tas{year}'))\n",
    "    \n",
    "    longitudinal_df = longitudinal_df.loc[:, ~longitudinal_df.columns.duplicated()]\n",
    "    print(\"TAS merge complete.\")\n",
    "    return longitudinal_df\n",
    "\n",
    "# --- STEP 4: Process 2007 Time Diary Data ---\n",
    "def process_time_diaries_2007():\n",
    "    \"\"\"\n",
    "    Creates a standalone DataFrame with comprehensive time-use variables for the 2007 wave.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 4: Processing 2007 Time Diary Data ---\")\n",
    "    \n",
    "    td_agg_df = load_data(os.path.join(CDS_2007_PATH, 'TD07_ACT_AGG.csv'))\n",
    "    td_activity_df = load_data(os.path.join(CDS_2007_PATH, 'TD_ACTIVITY07.csv'))\n",
    "\n",
    "    child_identifiers = td_agg_df[['AGGRID07', 'AGGRSN07']].copy().rename(columns={'AGGRID07': 'ID_2007', 'AGGRSN07': 'SN_2007'})\n",
    "\n",
    "    part_a_df = calculate_aggregate_weekly_hours_2007(child_identifiers.copy(), td_agg_df)\n",
    "    part_b_df = calculate_intensive_parenting_time_2007(td_activity_df)\n",
    "    \n",
    "    time_use_df = pd.merge(part_a_df, part_b_df, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    time_use_df.fillna(0, inplace=True)\n",
    "    print(\"2007 Time Diary processing complete.\")\n",
    "    return time_use_df\n",
    "\n",
    "def calculate_aggregate_weekly_hours_2007(base_df, td_agg_df):\n",
    "    \"\"\"Calculates weekly average hours for 2007.\"\"\"\n",
    "    td_agg_df.rename(columns={'AGGRID07': 'ID_2007', 'AGGRSN07': 'SN_2007'}, inplace=True)\n",
    "    panel_with_agg = pd.merge(base_df, td_agg_df, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    activity_codes = [f'39{i:02d}' for i in range(1, 40)]\n",
    "    for code in activity_codes:\n",
    "        wd_col, we_col = f'WD07{code}', f'WE07{code}'\n",
    "        new_col = f'weekly_avg_hrs_cat_{code}_07'\n",
    "        if wd_col in panel_with_agg.columns and we_col in panel_with_agg.columns:\n",
    "            wd_sec = panel_with_agg[wd_col].fillna(0)\n",
    "            we_sec = panel_with_agg[we_col].fillna(0)\n",
    "            panel_with_agg[new_col] = ((wd_sec * 5) + (we_sec * 2)) / 3600\n",
    "    new_cols = ['ID_2007', 'SN_2007'] + [f'weekly_avg_hrs_cat_{code}_07' for code in activity_codes]\n",
    "    return panel_with_agg[new_cols]\n",
    "\n",
    "def calculate_intensive_parenting_time_2007(td_activity_df):\n",
    "    \"\"\"Calculates 'intensive parenting' measures for 2007.\"\"\"\n",
    "    skill_codes = [5490, 5491, 5492, 5493, 5494, 8010, 8011, 8012, 5040, 8020, 8030, 8040, 8090, 8510, 8520, 8211, 8212, 8213, 8214, 8215, 8221, 8222, 8223]\n",
    "    skill_df = td_activity_df[td_activity_df['COLA_07'].isin(skill_codes)].copy()\n",
    "    wd_skill_df = skill_df[skill_df['DIARY_07'] == 0]\n",
    "    we_skill_df = skill_df[skill_df['DIARY_07'] == 1]\n",
    "    \n",
    "    child_ids = td_activity_df[['TDID07', 'TDSN07']].drop_duplicates().rename(columns={'TDID07': 'ID_2007', 'TDSN07': 'SN_2007'})\n",
    "\n",
    "    for day_type, df in [('wd', wd_skill_df), ('we', we_skill_df)]:\n",
    "        for parent, col in [('mother', 'COLGB_07'), ('father', 'COLGC_07')]:\n",
    "            mask = df[col] == 1\n",
    "            time = df[mask].groupby(['TDID07', 'TDSN07'])['DUR_07'].sum().reset_index()\n",
    "            time.rename(columns={'DUR_07': f'{parent}_interactive_{day_type}_sec_07', 'TDID07': 'ID_2007', 'TDSN07': 'SN_2007'}, inplace=True)\n",
    "            child_ids = pd.merge(child_ids, time, on=['ID_2007', 'SN_2007'], how='left')\n",
    "\n",
    "    cols_to_fill = [f'{p}_interactive_{d}_sec_07' for p in ['mother', 'father'] for d in ['wd', 'we']]\n",
    "    for col in cols_to_fill:\n",
    "        if col not in child_ids.columns: child_ids[col] = 0\n",
    "        else: child_ids[col] = child_ids[col].fillna(0)\n",
    "            \n",
    "    child_ids['parent_interactive_skill_hrs_wk_07'] = \\\n",
    "        (((child_ids['mother_interactive_wd_sec_07'] + child_ids['father_interactive_wd_sec_07']) * 5) +\n",
    "         ((child_ids['mother_interactive_we_sec_07'] + child_ids['father_interactive_we_sec_07']) * 2)) / 3600\n",
    "         \n",
    "    return child_ids[['ID_2007', 'SN_2007', 'parent_interactive_skill_hrs_wk_07']]\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(ANALYSIS_PATH):\n",
    "        os.makedirs(ANALYSIS_PATH)\n",
    "\n",
    "    # Step 1\n",
    "    core_cds_df = merge_core_cds_data_2007()\n",
    "    path_step1 = os.path.join(ANALYSIS_PATH, '01_cds_merged_2007.csv')\n",
    "    core_cds_df.to_csv(path_step1, index=False)\n",
    "    print(f\"Step 1 intermediate file saved to: {path_step1}\")\n",
    "\n",
    "    # Step 2\n",
    "    cds_family_df = merge_family_file_2007(core_cds_df)\n",
    "    path_step2 = os.path.join(ANALYSIS_PATH, '02_cds_with_family_data_2007.csv')\n",
    "    cds_family_df.to_csv(path_step2, index=False)\n",
    "    print(f\"Step 2 intermediate file saved to: {path_step2}\")\n",
    "\n",
    "    # Step 3\n",
    "    cds_tas_panel = merge_longitudinal_tas_data(cds_family_df)\n",
    "    path_step3 = os.path.join(ANALYSIS_PATH, '03_cds_tas_panel_2007.csv')\n",
    "    cds_tas_panel.to_csv(path_step3, index=False)\n",
    "    print(f\"Step 3 intermediate file saved to: {path_step3}\")\n",
    "\n",
    "    # Step 4\n",
    "    time_use_variables = process_time_diaries_2007()\n",
    "    path_step4 = os.path.join(ANALYSIS_PATH, '04_time_use_variables_2007.csv')\n",
    "    time_use_variables.to_csv(path_step4, index=False)\n",
    "    print(f\"Step 4 intermediate file saved to: {path_step4}\")\n",
    "\n",
    "    # Step 5: Final Merge\n",
    "    print(\"\\n--- Step 5: Final Merge ---\")\n",
    "    final_dataset = pd.merge(cds_tas_panel, time_use_variables, on=['ID_2007', 'SN_2007'], how='left')\n",
    "    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated()]\n",
    "    print(\"All 2007 data sources successfully merged.\")\n",
    "\n",
    "    # Save final outputs\n",
    "    final_path = os.path.join(ANALYSIS_PATH, 'final_analysis_dataset_2007.csv')\n",
    "    final_dataset.to_csv(final_path, index=False)\n",
    "    print(f\"Final dataset saved to: {final_path}\")\n",
    "    \n",
    "    if len(final_dataset) >= 1000:\n",
    "        sample_df = final_dataset.sample(n=1000, random_state=42)\n",
    "        sample_path = os.path.join(ANALYSIS_PATH, 'sample_final_analysis_dataset_2007.csv')\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        print(f\"Sample dataset saved to: {sample_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
