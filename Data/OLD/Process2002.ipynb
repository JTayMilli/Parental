{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27b35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Iterable, List, Optional, Tuple\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     33\u001b[0m BASE_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjoshu\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAussie\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMonash\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mParental\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:151\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    134\u001b[0m     concat,\n\u001b[0;32m    135\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     qcut,\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     ExcelFile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m     read_spss,\n\u001b[0;32m    185\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\testing.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mPublic testing utility functions.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[0;32m      8\u001b[0m     assert_frame_equal,\n\u001b[0;32m      9\u001b[0m     assert_index_equal,\n\u001b[0;32m     10\u001b[0m     assert_series_equal,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_extension_array_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_frame_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_series_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_index_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\_testing\\__init__.py:44\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     round_trip_localpath,\n\u001b[0;32m     36\u001b[0m     round_trip_pathlib,\n\u001b[0;32m     37\u001b[0m     round_trip_pickle,\n\u001b[0;32m     38\u001b[0m     write_to_compressed,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     assert_produces_warning,\n\u001b[0;32m     42\u001b[0m     maybe_produces_warning,\n\u001b[0;32m     43\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masserters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     assert_almost_equal,\n\u001b[0;32m     46\u001b[0m     assert_attr_equal,\n\u001b[0;32m     47\u001b[0m     assert_categorical_equal,\n\u001b[0;32m     48\u001b[0m     assert_class_equal,\n\u001b[0;32m     49\u001b[0m     assert_contains_all,\n\u001b[0;32m     50\u001b[0m     assert_copy,\n\u001b[0;32m     51\u001b[0m     assert_datetime_array_equal,\n\u001b[0;32m     52\u001b[0m     assert_dict_equal,\n\u001b[0;32m     53\u001b[0m     assert_equal,\n\u001b[0;32m     54\u001b[0m     assert_extension_array_equal,\n\u001b[0;32m     55\u001b[0m     assert_frame_equal,\n\u001b[0;32m     56\u001b[0m     assert_index_equal,\n\u001b[0;32m     57\u001b[0m     assert_indexing_slices_equivalent,\n\u001b[0;32m     58\u001b[0m     assert_interval_array_equal,\n\u001b[0;32m     59\u001b[0m     assert_is_sorted,\n\u001b[0;32m     60\u001b[0m     assert_is_valid_plot_return_object,\n\u001b[0;32m     61\u001b[0m     assert_metadata_equivalent,\n\u001b[0;32m     62\u001b[0m     assert_numpy_array_equal,\n\u001b[0;32m     63\u001b[0m     assert_period_array_equal,\n\u001b[0;32m     64\u001b[0m     assert_series_equal,\n\u001b[0;32m     65\u001b[0m     assert_sp_array_equal,\n\u001b[0;32m     66\u001b[0m     assert_timedelta_array_equal,\n\u001b[0;32m     67\u001b[0m     raise_assert_detail,\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     70\u001b[0m     get_dtype,\n\u001b[0;32m     71\u001b[0m     get_obj,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontexts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     74\u001b[0m     assert_cow_warning,\n\u001b[0;32m     75\u001b[0m     decompress_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     with_csv_dialect,\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pandas\\_testing\\asserters.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_matching_na\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseIndex\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_testing\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp_datetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compare_mismatched_resolutions\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     is_bool,\n\u001b[0;32m     21\u001b[0m     is_float_dtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     needs_i8_conversion,\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#   1. Merge all core 2002 CDS files and the Parent Identification File (PID)\n",
    "#      to create a single cross-sectional dataset with permanent IDs.\n",
    "#   2. Merge the 2001 PSID Family File to enrich the 2002 baseline.\n",
    "#   3. Merge the longitudinal Transition to Adulthood (TAS) waves (2005 and 2015).\n",
    "#   4. Process all 2002 Time Diary data to create both aggregate and contextual variables.\n",
    "#   5. Perform the final merge to combine all data sources into a single analysis file.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration: Define all base paths for the 2002 data processing ---\n",
    "BASE_DATA_PATH = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data'\n",
    "CDS_2002_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Child Development Survey', 'CDS2002', '2002')\n",
    "TAS_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Transition into Adulthood Supplement')\n",
    "FAMILY_FILES_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Family Files')\n",
    "# Path for the master Parent Identification File, used to get permanent longitudinal IDs.\n",
    "PID_FILE_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Parent Identification 2023') \n",
    "ANALYSIS_PATH = os.path.join(BASE_DATA_PATH, 'Processed Data 2002')\n",
    "\n",
    "# --- Helper Function to Load Data ---\n",
    "def load_data(file_path, required=True):\n",
    "    \"\"\"Safely loads a CSV file, printing its status and shape.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"  - Successfully loaded: {os.path.basename(file_path)} (Shape: {df.shape})\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        if required:\n",
    "            print(f\"  - FATAL ERROR: Required file not found at {file_path}\")\n",
    "            raise\n",
    "        else:\n",
    "            print(f\"  - Warning: Optional file not found, skipping: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "\n",
    "# --- STEP 1: Merge Core CDS-II Data (2002 Wave) ---\n",
    "def merge_core_cds_data_2002():\n",
    "    \"\"\"\n",
    "    Loads and merges all raw 2002 CDS files. This now includes merging both the PID file and\n",
    "    the original GEN_MAP file to ensure permanent identifiers are robustly included.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Merging Core CDS-II Data (2002 Wave) ---\")\n",
    "    \n",
    "    # Load all necessary raw files for 2002\n",
    "    demog_df = load_data(os.path.join(CDS_2002_PATH, 'DEMOG.csv'))\n",
    "    gen_map_df = load_data(os.path.join(CDS_2002_PATH, 'GEN_MAP.csv'), required=False)\n",
    "    pcg_chld_df = load_data(os.path.join(CDS_2002_PATH, 'PCG_CHLD.csv'))\n",
    "    child_df = load_data(os.path.join(CDS_2002_PATH, 'CHILD.csv'), required=False)\n",
    "    assessmt_df = load_data(os.path.join(CDS_2002_PATH, 'ASSESSMT.csv'), required=False)\n",
    "    ocg_chld_df = load_data(os.path.join(CDS_2002_PATH, 'OCG_CHLD.csv'), required=False)\n",
    "    idmap_df = load_data(os.path.join(CDS_2002_PATH, 'IDMAP02.csv'))\n",
    "    pcg_hhld_df = load_data(os.path.join(CDS_2002_PATH, 'PCG_HHLD.csv'))\n",
    "    pid_df = load_data(os.path.join(PID_FILE_PATH, 'PID23.csv'))\n",
    "\n",
    "    # Prepare base dataframe and standardize identifiers\n",
    "    demog_df.rename(columns={'DEMID01': 'ID_2001', 'DEMSN01': 'SN_2001'}, inplace=True)\n",
    "    merged_df = demog_df\n",
    "    \n",
    "    # Merge PID file to add permanent IDs. PID8/PID9 correspond to 2001 identifiers.\n",
    "    pid_subset = pid_df[['PID2', 'PID3', 'PID8', 'PID9']].copy()\n",
    "    pid_subset.rename(columns={'PID2': 'ER30001_pid', 'PID3': 'ER30002_pid', 'PID8': 'ID_2001', 'PID9': 'SN_2001'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, pid_subset, on=['ID_2001', 'SN_2001'], how='left')\n",
    "\n",
    "    # Also merge GEN_MAP to retain original data structure and provide a cross-check.\n",
    "    if gen_map_df is not None:\n",
    "        gen_map_df.rename(columns={'GENID01': 'ID_2001', 'GENSN01': 'SN_2001', 'CH_ID68': 'ER30001', 'CH_PN': 'ER30002'}, inplace=True)\n",
    "        merged_df = pd.merge(merged_df, gen_map_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    \n",
    "    # Merge all child-level files\n",
    "    child_files = {'pcg_chld': pcg_chld_df, 'child': child_df, 'assessmt': assessmt_df, 'ocg_chld': ocg_chld_df}\n",
    "    key_map = {'pcg_chld': ('PCHID01', 'PCHSN01'), 'child': ('CHLDID01', 'CHLDSN01'), 'assessmt': ('ASMTID01', 'ASMTSN01'), 'ocg_chld': ('OCGCID01', 'OCGCSN01')}\n",
    "    \n",
    "    for name, df in child_files.items():\n",
    "        if df is not None:\n",
    "            key_id, key_sn = key_map[name]\n",
    "            df.rename(columns={key_id: 'ID_2001', key_sn: 'SN_2001'}, inplace=True)\n",
    "            merged_df = pd.merge(merged_df, df, on=['ID_2001', 'SN_2001'], how='left', suffixes=('', f'_{name}'))\n",
    "            \n",
    "    # Merge household data via IDMAP\n",
    "    idmap_df.rename(columns={'CHLDID02': 'ID_2001', 'CHLDSN02': 'SN_2001'}, inplace=True)\n",
    "    pcg_hhld_df.rename(columns={'PHHID01': 'PCGID02', 'PHHSN01': 'PCGSN02'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, idmap_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    merged_df = pd.merge(merged_df, pcg_hhld_df, on=['PCGID02', 'PCGSN02'], how='left', suffixes=('', '_pcghhld'))\n",
    "    \n",
    "    # cleanup\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(\"Core CDS merge complete.\")\n",
    "    return merged_df\n",
    "\n",
    "# --- STEP 2: Merge 2001 PSID Family File ---\n",
    "def merge_family_file_2002(base_df):\n",
    "    \"\"\"\n",
    "    Merges the 2001 PSID Family File, adding household-level economic and social variables.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 2: Merging 2001 PSID Family File ---\")\n",
    "    family_df = load_data(os.path.join(FAMILY_FILES_PATH, 'fam2001er', 'FAM2001ER.csv'))\n",
    "    \n",
    "    # The key in the family file is 'ER17002', which corresponds to 'ID_2001'\n",
    "    family_df.rename(columns={'ER17002': 'ID_2001'}, inplace=True)\n",
    "    \n",
    "    enriched_df = pd.merge(base_df, family_df, on='ID_2001', how='left', suffixes=('', '_fam2001'))\n",
    "    \n",
    "    enriched_df = enriched_df.loc[:, ~enriched_df.columns.duplicated()]\n",
    "    print(\"Family file merge complete.\")\n",
    "    return enriched_df\n",
    "\n",
    "# --- STEP 3: Merge Longitudinal TAS Data ---\n",
    "def merge_longitudinal_tas_data(base_df):\n",
    "    \"\"\"\n",
    "    Merges the Transition to Adulthood (TAS) waves onto the base CDS dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 3: Merging Longitudinal TAS Data ---\")\n",
    "    \n",
    "    tas_files = {\n",
    "        '05': (os.path.join(TAS_PATH, 'ta2005', 'TA2005.csv'), 'TA050004', 'TA050005'),\n",
    "        '15': (os.path.join(TAS_PATH, 'ta2015', 'TA2015.csv'), 'TA150004', 'TA150005')\n",
    "    }\n",
    "    \n",
    "    longitudinal_df = base_df.copy()\n",
    "    for year, (path, id_col, pn_col) in tas_files.items():\n",
    "        tas_df = load_data(path, required=False)\n",
    "        if tas_df is not None:\n",
    "            tas_df.rename(columns={id_col: 'ER30001', pn_col: 'ER30002'}, inplace=True)\n",
    "            longitudinal_df = pd.merge(longitudinal_df, tas_df, on=['ER30001', 'ER30002'], how='left', suffixes=('', f'_tas{year}'))\n",
    "    \n",
    "    longitudinal_df = longitudinal_df.loc[:, ~longitudinal_df.columns.duplicated()]\n",
    "    print(\"TAS merge complete.\")\n",
    "    return longitudinal_df\n",
    "\n",
    "# --- STEP 4: Process 2002 Time Diary Data ---\n",
    "def process_time_diaries_2002():\n",
    "    \"\"\"\n",
    "    Creates a standalone DataFrame with comprehensive time-use variables from the 2002 data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 4: Processing Time Diary Data ---\")\n",
    "    \n",
    "    td_agg_df = load_data(os.path.join(CDS_2002_PATH, 'TD02_ACT_AGG.csv'))\n",
    "    td_activity_df = load_data(os.path.join(CDS_2002_PATH, 'TD_ACTIVITY.csv'))\n",
    "\n",
    "    child_identifiers = td_agg_df[['AGGRID01', 'AGGRSN01']].copy().rename(columns={'AGGRID01': 'ID_2001', 'AGGRSN01': 'SN_2001'})\n",
    "\n",
    "    part_a_df = calculate_aggregate_weekly_hours_2002(child_identifiers.copy(), td_agg_df)\n",
    "    part_b_df = calculate_intensive_parenting_time_2002(td_activity_df)\n",
    "    \n",
    "    time_use_df = pd.merge(part_a_df, part_b_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    time_use_df.fillna(0, inplace=True)\n",
    "    print(\"Time Diary processing complete.\")\n",
    "    return time_use_df\n",
    "\n",
    "def calculate_aggregate_weekly_hours_2002(base_df, td_agg_df):\n",
    "    \"\"\"Calculates weekly average hours for 39 broad activity categories.\"\"\"\n",
    "    td_agg_df.rename(columns={'AGGRID01': 'ID_2001', 'AGGRSN01': 'SN_2001'}, inplace=True)\n",
    "    panel_with_agg = pd.merge(base_df, td_agg_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    activity_codes = [f'39{i:02d}' for i in range(1, 40)]\n",
    "    for code in activity_codes:\n",
    "        wd_col, we_col = f'WD02{code}', f'WE02{code}'\n",
    "        new_col = f'weekly_avg_hrs_cat_{code}'\n",
    "        if wd_col in panel_with_agg.columns and we_col in panel_with_agg.columns:\n",
    "            wd_sec = panel_with_agg[wd_col].fillna(0)\n",
    "            we_sec = panel_with_agg[we_col].fillna(0)\n",
    "            panel_with_agg[new_col] = ((wd_sec * 5) + (we_sec * 2)) / 3600\n",
    "    new_cols = ['ID_2001', 'SN_2001'] + [f'weekly_avg_hrs_cat_{code}' for code in activity_codes]\n",
    "    return panel_with_agg[[col for col in new_cols if col in panel_with_agg.columns]]\n",
    "\n",
    "def calculate_intensive_parenting_time_2002(td_activity_df):\n",
    "    \"\"\"Processes the raw activity file to calculate measures of 'intensive parenting'.\"\"\"\n",
    "    skill_codes = [5490, 5491, 5492, 5493, 5494, 8010, 8011, 8012, 5040, 8020, 8030, 8040, 8090, 8510, 8520, 8211, 8212, 8213, 8214, 8215, 8221, 8222, 8223]\n",
    "    skill_df = td_activity_df[td_activity_df['COLA_02'].isin(skill_codes)].copy()\n",
    "    wd_skill_df = skill_df[skill_df['DIARY_02'] == 0]\n",
    "    we_skill_df = skill_df[skill_df['DIARY_02'] == 1]\n",
    "    \n",
    "    child_ids = td_activity_df[['TDID01', 'TDSN01']].drop_duplicates().rename(columns={'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'})\n",
    "\n",
    "    for day_type, df in [('wd', wd_skill_df), ('we', we_skill_df)]:\n",
    "        for parent, col in [('mother', 'COLGB_02'), ('father', 'COLGC_02')]:\n",
    "            mask = df[col] == 1\n",
    "            time = df[mask].groupby(['TDID01', 'TDSN01'])['DUR_02'].sum().reset_index()\n",
    "            time.rename(columns={'DUR_02': f'{parent}_interactive_{day_type}_sec', 'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'}, inplace=True)\n",
    "            child_ids = pd.merge(child_ids, time, on=['ID_2001', 'SN_2001'], how='left')\n",
    "\n",
    "    cols_to_fill = [f'{p}_interactive_{d}_sec' for p in ['mother', 'father'] for d in ['wd', 'we']]\n",
    "    for col in cols_to_fill:\n",
    "        if col not in child_ids.columns: child_ids[col] = 0\n",
    "        else: child_ids[col] = child_ids[col].fillna(0)\n",
    "            \n",
    "    child_ids['parent_interactive_skill_hrs_wk'] = \\\n",
    "        (((child_ids['mother_interactive_wd_sec'] + child_ids['father_interactive_wd_sec']) * 5) +\n",
    "         ((child_ids['mother_interactive_we_sec'] + child_ids['father_interactive_we_sec']) * 2)) / 3600\n",
    "         \n",
    "    return child_ids[['ID_2001', 'SN_2001', 'parent_interactive_skill_hrs_wk']]\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # Create the analysis folder if it doesn't exist\n",
    "    if not os.path.exists(ANALYSIS_PATH):\n",
    "        os.makedirs(ANALYSIS_PATH)\n",
    "\n",
    "    # Step 1\n",
    "    core_cds_df = merge_core_cds_data_2002()\n",
    "    path_step1 = os.path.join(ANALYSIS_PATH, '01_cds_merged.csv')\n",
    "    core_cds_df.to_csv(path_step1, index=False)\n",
    "    print(f\"Step 1 intermediate file saved to: {path_step1}\")\n",
    "\n",
    "    # Step 2\n",
    "    cds_family_df = merge_family_file_2002(core_cds_df)\n",
    "    path_step2 = os.path.join(ANALYSIS_PATH, '02_cds_with_family_data.csv')\n",
    "    cds_family_df.to_csv(path_step2, index=False)\n",
    "    print(f\"Step 2 intermediate file saved to: {path_step2}\")\n",
    "\n",
    "    # Step 3\n",
    "    cds_tas_panel = merge_longitudinal_tas_data(cds_family_df)\n",
    "    path_step3 = os.path.join(ANALYSIS_PATH, '03_cds_tas_panel.csv')\n",
    "    cds_tas_panel.to_csv(path_step3, index=False)\n",
    "    print(f\"Step 3 intermediate file saved to: {path_step3}\")\n",
    "\n",
    "    # Step 4\n",
    "    time_use_variables = process_time_diaries_2002()\n",
    "    path_step4 = os.path.join(ANALYSIS_PATH, '04_time_use_variables.csv')\n",
    "    time_use_variables.to_csv(path_step4, index=False)\n",
    "    print(f\"Step 4 intermediate file saved to: {path_step4}\")\n",
    "\n",
    "    # Step 5: Final Merge\n",
    "    print(\"\\n--- Step 5: Final Merge ---\")\n",
    "    final_dataset = pd.merge(cds_tas_panel, time_use_variables, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated()]\n",
    "    print(\"All data sources successfully merged.\")\n",
    "\n",
    "    # Save final outputs\n",
    "    final_path = os.path.join(ANALYSIS_PATH, 'final_analysis_dataset.csv')\n",
    "    final_dataset.to_csv(final_path, index=False)\n",
    "    print(f\"Final dataset saved to: {final_path}\")\n",
    "    \n",
    "    if len(final_dataset) >= 1000:\n",
    "        sample_df = final_dataset.sample(n=1000, random_state=42)\n",
    "        sample_path = os.path.join(ANALYSIS_PATH, 'sample_final_analysis_dataset.csv')\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        print(f\"Sample dataset saved to: {sample_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
