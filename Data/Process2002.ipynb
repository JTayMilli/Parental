{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Merging Core CDS-II Data (2002 Wave) ---\n",
      "  - Successfully loaded: DEMOG.csv (Shape: (2907, 22))\n",
      "  - Successfully loaded: GEN_MAP.csv (Shape: (2907, 9))\n",
      "  - Successfully loaded: PCG_CHLD.csv (Shape: (2907, 921))\n",
      "  - Successfully loaded: CHILD.csv (Shape: (2182, 508))\n",
      "  - Successfully loaded: ASSESSMT.csv (Shape: (2644, 256))\n",
      "  - Successfully loaded: OCG_CHLD.csv (Shape: (1686, 79))\n",
      "  - Successfully loaded: IDMAP02.csv (Shape: (2891, 8))\n",
      "  - Successfully loaded: PCG_HHLD.csv (Shape: (2009, 218))\n",
      "Core CDS merge complete.\n",
      "Step 1 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data\\01_cds_merged.csv\n",
      "\n",
      "--- Step 2: Merging 2001 PSID Family File ---\n",
      "  - Successfully loaded: FAM2001ER.csv (Shape: (7406, 3559))\n",
      "Family file merge complete.\n",
      "Step 2 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data\\02_cds_with_family_data.csv\n",
      "\n",
      "--- Step 3: Merging Longitudinal TAS Data ---\n",
      "  - Successfully loaded: TA2005.csv (Shape: (745, 961))\n",
      "  - Successfully loaded: TA2015.csv (Shape: (1641, 1306))\n",
      "TAS merge complete.\n",
      "Step 3 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data\\03_cds_tas_panel.csv\n",
      "\n",
      "--- Step 4: Processing Time Diary Data ---\n",
      "  - Successfully loaded: TD02_ACT_AGG.csv (Shape: (2569, 1311))\n",
      "  - Successfully loaded: TD_ACTIVITY.csv (Shape: (99467, 34))\n",
      "Time Diary processing complete.\n",
      "Step 4 intermediate file saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data\\04_time_use_variables.csv\n",
      "\n",
      "--- Step 5: Final Merge ---\n",
      "All data sources successfully merged.\n",
      "Final dataset saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data\\final_analysis_dataset.csv\n",
      "Sample dataset saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Processed Data\\sample_final_analysis_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#   1. Merge all core 2002 CDS files into a single cross-sectional dataset.\n",
    "#   2. Merge the 2001 PSID Family File to enrich the 2002 baseline.\n",
    "#   3. Merge the longitudinal Transition to Adulthood (TAS) waves.\n",
    "#   4. Process all Time Diary data to create both aggregate and contextual variables.\n",
    "#   5. Perform the final merge to combine all data sources.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define all base paths\n",
    "BASE_DATA_PATH = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data'\n",
    "CDS_2002_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Child Development Survey', 'CDS2002', '2002')\n",
    "TAS_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Transition into Adulthood Supplement')\n",
    "FAMILY_FILES_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Family Files')\n",
    "ANALYSIS_PATH = os.path.join(BASE_DATA_PATH, 'Processed Data')\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path, required=True):\n",
    "    \"\"\"Safely loads a CSV file, print status and shape.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"  - Successfully loaded: {os.path.basename(file_path)} (Shape: {df.shape})\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        if required:\n",
    "            print(f\"  - FATAL ERROR: Required file not found at {file_path}\")\n",
    "            raise\n",
    "        else:\n",
    "            print(f\"  - Warning: Optional file not found, skipping: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "\n",
    "# Merge Core CDS-II Data\n",
    "def merge_core_cds_data():\n",
    "    \"\"\"\n",
    "    Raw 2002 CDS files into a single cross-sectional dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Merging Core CDS-II Data (2002 Wave) ---\")\n",
    "    \n",
    "    # Load all necessary raw files\n",
    "    demog_df = load_data(os.path.join(CDS_2002_PATH, 'DEMOG.csv'))\n",
    "    gen_map_df = load_data(os.path.join(CDS_2002_PATH, 'GEN_MAP.csv'))\n",
    "    pcg_chld_df = load_data(os.path.join(CDS_2002_PATH, 'PCG_CHLD.csv'))\n",
    "    child_df = load_data(os.path.join(CDS_2002_PATH, 'CHILD.csv'), required=False)\n",
    "    assessmt_df = load_data(os.path.join(CDS_2002_PATH, 'ASSESSMT.csv'), required=False)\n",
    "    ocg_chld_df = load_data(os.path.join(CDS_2002_PATH, 'OCG_CHLD.csv'), required=False)\n",
    "    idmap_df = load_data(os.path.join(CDS_2002_PATH, 'IDMAP02.csv'))\n",
    "    pcg_hhld_df = load_data(os.path.join(CDS_2002_PATH, 'PCG_HHLD.csv'))\n",
    "\n",
    "    # Rename keys for consistency before merging\n",
    "    demog_df.rename(columns={'DEMID01': 'ID_2001', 'DEMSN01': 'SN_2001'}, inplace=True)\n",
    "    gen_map_df.rename(columns={'GENID01': 'ID_2001', 'GENSN01': 'SN_2001', 'CH_ID68': 'ER30001', 'CH_PN': 'ER30002'}, inplace=True)\n",
    "    \n",
    "    # Start with the demographic file as the base\n",
    "    merged_df = pd.merge(demog_df, gen_map_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    \n",
    "    # Merge all child-level files\n",
    "    child_files = {'pcg_chld': pcg_chld_df, 'child': child_df, 'assessmt': assessmt_df, 'ocg_chld': ocg_chld_df}\n",
    "    key_map = {'pcg_chld': ('PCHID01', 'PCHSN01'), 'child': ('CHLDID01', 'CHLDSN01'), 'assessmt': ('ASMTID01', 'ASMTSN01'), 'ocg_chld': ('OCGCID01', 'OCGCSN01')}\n",
    "    \n",
    "    for name, df in child_files.items():\n",
    "        if df is not None:\n",
    "            key_id, key_sn = key_map[name]\n",
    "            df.rename(columns={key_id: 'ID_2001', key_sn: 'SN_2001'}, inplace=True)\n",
    "            merged_df = pd.merge(merged_df, df, on=['ID_2001', 'SN_2001'], how='left', suffixes=('', f'_{name}'))\n",
    "            \n",
    "    # Merge household data via IDMAP\n",
    "    idmap_df.rename(columns={'CHLDID02': 'ID_2001', 'CHLDSN02': 'SN_2001'}, inplace=True)\n",
    "    pcg_hhld_df.rename(columns={'PHHID01': 'PCGID02', 'PHHSN01': 'PCGSN02'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, idmap_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    merged_df = pd.merge(merged_df, pcg_hhld_df, on=['PCGID02', 'PCGSN02'], how='left', suffixes=('', '_pcghhld'))\n",
    "    \n",
    "    # cleanup\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(\"Core CDS merge complete.\")\n",
    "    return merged_df\n",
    "\n",
    "# Merge 2001 PSID Family File\n",
    "def merge_family_file(base_df):\n",
    "    \"\"\"\n",
    "    2001 PSID Family File, adding\n",
    "    household-level economic and social variables.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 2: Merging 2001 PSID Family File ---\")\n",
    "    family_df = load_data(os.path.join(FAMILY_FILES_PATH, 'fam2001er', 'FAM2001ER.csv'))\n",
    "    \n",
    "    # The key in the family file is 'ER17002', which corresponds to 'ID_2001'\n",
    "    family_df.rename(columns={'ER17002': 'ID_2001'}, inplace=True)\n",
    "    \n",
    "    enriched_df = pd.merge(base_df, family_df, on='ID_2001', how='left', suffixes=('', '_fam2001'))\n",
    "    \n",
    "    enriched_df = enriched_df.loc[:, ~enriched_df.columns.duplicated()]\n",
    "    print(\"Family file merge complete.\")\n",
    "    return enriched_df\n",
    "\n",
    "# Merge Longitudinal TAS Data \n",
    "def merge_longitudinal_tas_data(base_df):\n",
    "    \"\"\"\n",
    "    Merges the Transition to Adulthood (TAS) waves onto the base CDS dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 3: Merging Longitudinal TAS Data ---\")\n",
    "    \n",
    "    tas_files = {\n",
    "        '05': (os.path.join(TAS_PATH, 'ta2005', 'TA2005.csv'), 'TA050004', 'TA050005'),\n",
    "        '15': (os.path.join(TAS_PATH, 'ta2015', 'TA2015.csv'), 'TA150004', 'TA150005')\n",
    "    }\n",
    "    \n",
    "    longitudinal_df = base_df.copy()\n",
    "    for year, (path, id_col, pn_col) in tas_files.items():\n",
    "        tas_df = load_data(path, required=False)\n",
    "        if tas_df is not None:\n",
    "            tas_df.rename(columns={id_col: 'ER30001', pn_col: 'ER30002'}, inplace=True)\n",
    "            longitudinal_df = pd.merge(longitudinal_df, tas_df, on=['ER30001', 'ER30002'], how='left', suffixes=('', f'_tas{year}'))\n",
    "    \n",
    "    longitudinal_df = longitudinal_df.loc[:, ~longitudinal_df.columns.duplicated()]\n",
    "    print(\"TAS merge complete.\")\n",
    "    return longitudinal_df\n",
    "\n",
    "# Process Time Diary Data \n",
    "def process_time_diaries():\n",
    "    \"\"\"\n",
    "    Creates a standalone DataFrame with comprehensive time-use variables.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 4: Processing Time Diary Data ---\")\n",
    "    \n",
    "    td_agg_df = load_data(os.path.join(CDS_2002_PATH, 'TD02_ACT_AGG.csv'))\n",
    "    td_activity_df = load_data(os.path.join(CDS_2002_PATH, 'TD_ACTIVITY.csv'))\n",
    "\n",
    "    child_identifiers = td_agg_df[['AGGRID01', 'AGGRSN01']].copy().rename(columns={'AGGRID01': 'ID_2001', 'AGGRSN01': 'SN_2001'})\n",
    "\n",
    "    # Part A: Aggregate weekly hours\n",
    "    part_a_df = calculate_aggregate_weekly_hours(child_identifiers.copy(), td_agg_df)\n",
    "    \n",
    "    # Part B: Intensive parenting time\n",
    "    part_b_df = calculate_intensive_parenting_time(td_activity_df)\n",
    "    \n",
    "    # Combine into a single time-use dataset\n",
    "    time_use_df = pd.merge(part_a_df, part_b_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    time_use_df.fillna(0, inplace=True)\n",
    "    print(\"Time Diary processing complete.\")\n",
    "    return time_use_df\n",
    "\n",
    "def calculate_aggregate_weekly_hours(base_df, td_agg_df):\n",
    "    \"\"\"Calculates weekly average hours for 39 broad activity categories.\"\"\"\n",
    "    td_agg_df.rename(columns={'AGGRID01': 'ID_2001', 'AGGRSN01': 'SN_2001'}, inplace=True)\n",
    "    panel_with_agg = pd.merge(base_df, td_agg_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    activity_codes = [f'39{i:02d}' for i in range(1, 40)]\n",
    "    for code in activity_codes:\n",
    "        wd_col, we_col = f'WD02{code}', f'WE02{code}'\n",
    "        new_col = f'weekly_avg_hrs_cat_{code}'\n",
    "        if wd_col in panel_with_agg.columns and we_col in panel_with_agg.columns:\n",
    "            wd_sec = panel_with_agg[wd_col].fillna(0)\n",
    "            we_sec = panel_with_agg[we_col].fillna(0)\n",
    "            panel_with_agg[new_col] = ((wd_sec * 5) + (we_sec * 2)) / 3600\n",
    "    new_cols = ['ID_2001', 'SN_2001'] + [f'weekly_avg_hrs_cat_{code}' for code in activity_codes]\n",
    "    return panel_with_agg[new_cols]\n",
    "\n",
    "def calculate_intensive_parenting_time(td_activity_df):\n",
    "    \"\"\"Processes the raw activity file to calculate measures of 'intensive parenting'.\"\"\"\n",
    "    skill_codes = [5490, 5491, 5492, 5493, 5494, 8010, 8011, 8012, 5040, 8020, 8030, 8040, 8090, 8510, 8520, 8211, 8212, 8213, 8214, 8215, 8221, 8222, 8223]\n",
    "    skill_df = td_activity_df[td_activity_df['COLA_02'].isin(skill_codes)].copy()\n",
    "    wd_skill_df = skill_df[skill_df['DIARY_02'] == 0]\n",
    "    we_skill_df = skill_df[skill_df['DIARY_02'] == 1]\n",
    "    \n",
    "    child_ids = td_activity_df[['TDID01', 'TDSN01']].drop_duplicates().rename(columns={'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'})\n",
    "\n",
    "    for day_type, df in [('wd', wd_skill_df), ('we', we_skill_df)]:\n",
    "        for parent, col in [('mother', 'COLGB_02'), ('father', 'COLGC_02')]:\n",
    "            mask = df[col] == 1\n",
    "            time = df[mask].groupby(['TDID01', 'TDSN01'])['DUR_02'].sum().reset_index()\n",
    "            time.rename(columns={'DUR_02': f'{parent}_interactive_{day_type}_sec', 'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'}, inplace=True)\n",
    "            child_ids = pd.merge(child_ids, time, on=['ID_2001', 'SN_2001'], how='left')\n",
    "\n",
    "    cols_to_fill = [f'{p}_interactive_{d}_sec' for p in ['mother', 'father'] for d in ['wd', 'we']]\n",
    "    for col in cols_to_fill:\n",
    "        if col not in child_ids.columns: child_ids[col] = 0\n",
    "        else: child_ids[col] = child_ids[col].fillna(0)\n",
    "            \n",
    "    child_ids['parent_interactive_skill_hrs_wk'] = \\\n",
    "        (((child_ids['mother_interactive_wd_sec'] + child_ids['father_interactive_wd_sec']) * 5) +\n",
    "         ((child_ids['mother_interactive_we_sec'] + child_ids['father_interactive_we_sec']) * 2)) / 3600\n",
    "         \n",
    "    return child_ids[['ID_2001', 'SN_2001', 'parent_interactive_skill_hrs_wk']]\n",
    "\n",
    "# Main Execution Block\n",
    "if __name__ == '__main__':\n",
    "    # Create the analysis folder if it doesn't exist\n",
    "    if not os.path.exists(ANALYSIS_PATH):\n",
    "        os.makedirs(ANALYSIS_PATH)\n",
    "\n",
    "    # Step 1\n",
    "    core_cds_df = merge_core_cds_data()\n",
    "    path_step1 = os.path.join(ANALYSIS_PATH, '01_cds_merged.csv')\n",
    "    core_cds_df.to_csv(path_step1, index=False)\n",
    "    print(f\"Step 1 intermediate file saved to: {path_step1}\")\n",
    "\n",
    "    # Step 2\n",
    "    cds_family_df = merge_family_file(core_cds_df)\n",
    "    path_step2 = os.path.join(ANALYSIS_PATH, '02_cds_with_family_data.csv')\n",
    "    cds_family_df.to_csv(path_step2, index=False)\n",
    "    print(f\"Step 2 intermediate file saved to: {path_step2}\")\n",
    "\n",
    "    # Step 3\n",
    "    cds_tas_panel = merge_longitudinal_tas_data(cds_family_df)\n",
    "    path_step3 = os.path.join(ANALYSIS_PATH, '03_cds_tas_panel.csv')\n",
    "    cds_tas_panel.to_csv(path_step3, index=False)\n",
    "    print(f\"Step 3 intermediate file saved to: {path_step3}\")\n",
    "\n",
    "    # Step 4\n",
    "    time_use_variables = process_time_diaries()\n",
    "    path_step4 = os.path.join(ANALYSIS_PATH, '04_time_use_variables.csv')\n",
    "    time_use_variables.to_csv(path_step4, index=False)\n",
    "    print(f\"Step 4 intermediate file saved to: {path_step4}\")\n",
    "\n",
    "    # Step 5: Final Merge\n",
    "    print(\"\\n--- Step 5: Final Merge ---\")\n",
    "    final_dataset = pd.merge(cds_tas_panel, time_use_variables, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated()]\n",
    "    print(\"All data sources successfully merged.\")\n",
    "\n",
    "    # Save final outputs\n",
    "    final_path = os.path.join(ANALYSIS_PATH, 'final_analysis_dataset.csv')\n",
    "    final_dataset.to_csv(final_path, index=False)\n",
    "    print(f\"Final dataset saved to: {final_path}\")\n",
    "    \n",
    "    if len(final_dataset) >= 1000:\n",
    "        sample_df = final_dataset.sample(n=1000, random_state=42)\n",
    "        sample_path = os.path.join(ANALYSIS_PATH, 'sample_final_analysis_dataset.csv')\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        print(f\"Sample dataset saved to: {sample_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
