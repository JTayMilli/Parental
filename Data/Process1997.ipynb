{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c16e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Merging Core CDS-I Data (1997 Wave) ---\n",
      "  - Successfully loaded: DEMOG1997.csv (Shape: (3563, 27))\n",
      "  - Successfully loaded: PCG97_CHLD.csv (Shape: (3563, 806))\n",
      "  - Successfully loaded: CHILD97.csv (Shape: (2223, 339))\n",
      "  - Successfully loaded: OCG_CHLD97.csv (Shape: (1395, 126))\n",
      "  - Successfully loaded: IDMAP97.csv (Shape: (2233, 6))\n",
      "  - Successfully loaded: PCG97_HH.csv (Shape: (1536, 311))\n",
      "  - Successfully loaded: PID23.csv (Shape: (103725, 40))\n",
      "Core 1997 CDS merge complete.\n"
     ]
    }
   ],
   "source": [
    "# This script processes the 1997 wave of the Panel Study of Income Dynamics (PSID) \n",
    "# Child Development Supplement (CDS-I) data. The process is designed to be analogous \n",
    "# to the processing for the 2002 and 2007 waves and follows five main steps:\n",
    "#\n",
    "#   1. Merge all core 1997 CDS files and the Parent Identification File (PID)\n",
    "#      to create a single cross-sectional dataset with permanent IDs.\n",
    "#   2. Merge the 1997 PSID Family File to enrich the 1997 baseline.\n",
    "#   3. Merge the longitudinal Transition to Adulthood (TAS) waves (2005 and 2015).\n",
    "#   4. Process all 1997 Time Diary data to create both aggregate and contextual variables.\n",
    "#   5. Perform the final merge to combine all data sources into a single analysis file.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration: Define all base paths for the 1997 data processing ---\n",
    "BASE_DATA_PATH = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data'\n",
    "CDS_1997_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Child Development Survey', 'CDS1997', '1997')\n",
    "TAS_PATH = os.path.join(BASE_DATA_PATH, 'Supplemental Studies', 'Transition into Adulthood Supplement')\n",
    "FAMILY_FILES_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Family Files')\n",
    "# Path for the master Parent Identification File, used to get permanent longitudinal IDs.\n",
    "PID_FILE_PATH = os.path.join(BASE_DATA_PATH, 'Main Study', 'Parent Identification 2023') \n",
    "ANALYSIS_PATH = os.path.join(BASE_DATA_PATH, 'Processed Data 1997')\n",
    "\n",
    "# --- Helper Function to Load Data ---\n",
    "def load_data(file_path, required=True):\n",
    "    \"\"\"Safely loads a CSV file, printing its status and shape.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"  - Successfully loaded: {os.path.basename(file_path)} (Shape: {df.shape})\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        if required:\n",
    "            print(f\"  - FATAL ERROR: Required file not found at {file_path}\")\n",
    "            raise\n",
    "        else:\n",
    "            print(f\"  - Warning: Optional file not found, skipping: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "\n",
    "# --- STEP 1: Merge Core CDS-I Data (1997 Wave) ---\n",
    "def merge_core_cds_data_1997():\n",
    "    \"\"\"\n",
    "    Loads and merges all raw 1997 CDS files into a single cross-sectional dataset.\n",
    "    This function also merges the master Parent Identification File (PID) to append\n",
    "    the permanent 1968 Family ID and Person Number, which are required for longitudinal merging.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Merging Core CDS-I Data (1997 Wave) ---\")\n",
    "    \n",
    "    # Load all necessary raw files for 1997\n",
    "    demog_df = load_data(os.path.join(CDS_1997_PATH, 'DEMOG1997.csv'))\n",
    "    pcg_chld_df = load_data(os.path.join(CDS_1997_PATH, 'PCG97_CHLD.csv'))\n",
    "    child_df = load_data(os.path.join(CDS_1997_PATH, 'CHILD97.csv'), required=False)\n",
    "    ocg_chld_df = load_data(os.path.join(CDS_1997_PATH, 'OCG_CHLD97.csv'), required=False)\n",
    "    idmap_df = load_data(os.path.join(CDS_1997_PATH, 'IDMAP97.csv'))\n",
    "    pcg_hhld_df = load_data(os.path.join(CDS_1997_PATH, 'PCG97_HH.csv'))\n",
    "    # Load the Parent Identification File to get permanent IDs, replacing the need for GENMAP97.\n",
    "    pid_df = load_data(os.path.join(PID_FILE_PATH, 'PID23.csv'))\n",
    "\n",
    "    # Prepare base dataframe by standardizing the 1997 child identifiers.\n",
    "    demog_df.rename(columns={'DEMID97': 'ID_1997', 'DEMSN97': 'SN_1997'}, inplace=True)\n",
    "    merged_df = demog_df\n",
    "    \n",
    "    # Prepare and merge the PID file to add permanent identifiers (ER30001, ER30002).\n",
    "    # PID4 is the 1997 Family ID and PID5 is the 1997 Person Number.\n",
    "    pid_subset = pid_df[['PID2', 'PID3', 'PID4', 'PID5']].copy()\n",
    "    pid_subset.rename(columns={'PID2': 'ER30001', 'PID3': 'ER30002', 'PID4': 'ID_1997', 'PID5': 'SN_1997'}, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, pid_subset, on=['ID_1997', 'SN_1997'], how='left')\n",
    "    \n",
    "    # Sequentially merge all child-level files.\n",
    "    child_files = {'pcg_chld97': pcg_chld_df, 'child97': child_df, 'ocg_chld97': ocg_chld_df}\n",
    "    key_map = {\n",
    "        'pcg_chld97': ('PCGCHID97', 'PCGCHSN97'), \n",
    "        'child97': ('CHLDID97', 'CHLDSN97'),\n",
    "        'ocg_chld97': ('OCGCID97', 'OCGCSN97')\n",
    "    }\n",
    "    \n",
    "    for name, df in child_files.items():\n",
    "        if df is not None:\n",
    "            key_id, key_sn = key_map[name]\n",
    "            if key_id in df.columns and key_sn in df.columns:\n",
    "                df.rename(columns={key_id: 'ID_1997', key_sn: 'SN_1997'}, inplace=True)\n",
    "                merged_df = pd.merge(merged_df, df, on=['ID_1997', 'SN_1997'], how='left', suffixes=('', f'_{name}'))\n",
    "            else:\n",
    "                print(f\"  - Warning: Key columns {key_id}, {key_sn} not found in {name}. Skipping merge.\")\n",
    "\n",
    "    # Merge household data via the IDMAP file to link PCG-reported household characteristics.\n",
    "    # The correct identifiers from the raw PCG97_HH.csv file are 'HHID97' and 'HHSN97'.\n",
    "    if 'HHID97' in pcg_hhld_df.columns and 'HHSN97' in pcg_hhld_df.columns:\n",
    "        pcg_hhld_df.rename(columns={'HHID97': 'PCGID97', 'HHSN97': 'PCGSN97'}, inplace=True)\n",
    "    \n",
    "        idmap_df.rename(columns={'CHILDID97': 'ID_1997', 'CHILDSN97': 'SN_1997'}, inplace=True)\n",
    "        \n",
    "        # Merge the ID map first to bring PCG keys into the main dataframe.\n",
    "        merged_df = pd.merge(merged_df, idmap_df, on=['ID_1997', 'SN_1997'], how='left')\n",
    "        # Now merge the PCG household data using the newly added PCG keys.\n",
    "        merged_df = pd.merge(merged_df, pcg_hhld_df, on=['PCGID97', 'PCGSN97'], how='left', suffixes=('', '_pcghh97'))\n",
    "    else:\n",
    "        print(\"  - Warning: PCG Household identifiers not found in PCG97_HH.csv. Skipping merge.\")\n",
    "\n",
    "    # Remove any duplicated columns that may have arisen from merges.\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(\"Core 1997 CDS merge complete.\")\n",
    "    return merged_df\n",
    "\n",
    "# --- STEP 2: Merge 1997 PSID Family File ---\n",
    "def merge_family_file_1997(base_df):\n",
    "    \"\"\"\n",
    "    Enriches the core CDS dataset by merging the 1997 PSID Family File. This adds\n",
    "    a wide range of household-level economic and social variables.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 2: Merging 1997 PSID Family File ---\")\n",
    "    family_df = load_data(os.path.join(FAMILY_FILES_PATH, 'fam1997er', 'FAM1997ER.csv'))\n",
    "    \n",
    "    # The key in the 1997 family file is 'ER10002', which corresponds to the 1997 Interview Number.\n",
    "    family_df.rename(columns={'ER10002': 'ID_1997'}, inplace=True)\n",
    "    \n",
    "    enriched_df = pd.merge(base_df, family_df, on='ID_1997', how='left', suffixes=('', '_fam1997'))\n",
    "    \n",
    "    enriched_df = enriched_df.loc[:, ~enriched_df.columns.duplicated()]\n",
    "    print(\"1997 Family file merge complete.\")\n",
    "    return enriched_df\n",
    "\n",
    "# --- STEP 3: Merge Longitudinal TAS Data ---\n",
    "def merge_longitudinal_tas_data(base_df):\n",
    "    \"\"\"\n",
    "    Merges the Transition to Adulthood (TAS) waves onto the base 1997 CDS dataset,\n",
    "    linking children to their later-life outcomes using the permanent PSID identifiers.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 3: Merging Longitudinal TAS Data ---\")\n",
    "    \n",
    "    tas_files = {\n",
    "        '05': (os.path.join(TAS_PATH, 'ta2005', 'TA2005.csv'), 'TA050004', 'TA050005'),\n",
    "        '15': (os.path.join(TAS_PATH, 'ta2015', 'TA2015.csv'), 'TA150004', 'TA150005')\n",
    "    }\n",
    "    \n",
    "    longitudinal_df = base_df.copy()\n",
    "    # Ensure the permanent IDs are present before attempting the merge.\n",
    "    if 'ER30001' not in longitudinal_df.columns or 'ER30002' not in longitudinal_df.columns:\n",
    "        print(\"  - Warning: Permanent IDs (ER30001, ER30002) not found. Cannot merge TAS data.\")\n",
    "        return longitudinal_df\n",
    "\n",
    "    for year, (path, id_col, pn_col) in tas_files.items():\n",
    "        tas_df = load_data(path, required=False)\n",
    "        if tas_df is not None:\n",
    "            tas_df.rename(columns={id_col: 'ER30001', pn_col: 'ER30002'}, inplace=True)\n",
    "            longitudinal_df = pd.merge(longitudinal_df, tas_df, on=['ER30001', 'ER30002'], how='left', suffixes=('', f'_tas{year}'))\n",
    "    \n",
    "    longitudinal_df = longitudinal_df.loc[:, ~longitudinal_df.columns.duplicated()]\n",
    "    print(\"TAS merge complete.\")\n",
    "    return longitudinal_df\n",
    "\n",
    "# --- STEP 4: Process 1997 Time Diary Data ---\n",
    "def process_time_diaries_1997():\n",
    "    \"\"\"\n",
    "    Creates a standalone DataFrame with comprehensive time-use variables for the 1997 wave,\n",
    "    including both aggregate weekly hours and specific measures of parental investment.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 4: Processing 1997 Time Diary Data ---\")\n",
    "    \n",
    "    # Load both aggregate and raw activity time diary files.\n",
    "    td_agg_df = load_data(os.path.join(CDS_1997_PATH, 'TD97_ACT_AGG.csv'), required=False) \n",
    "    td_activity_df = load_data(os.path.join(CDS_1997_PATH, 'TD97.csv'))\n",
    "\n",
    "    # Part A: Calculate aggregate weekly hours if the aggregate file is available.\n",
    "    part_a_df = None\n",
    "    if td_agg_df is not None:\n",
    "        child_identifiers_agg = td_agg_df[['AGGRID97', 'AGGRSN97']].copy().rename(columns={'AGGRID97': 'ID_1997', 'AGGRSN97': 'SN_1997'})\n",
    "        part_a_df = calculate_aggregate_weekly_hours_1997(child_identifiers_agg, td_agg_df)\n",
    "    \n",
    "    # Part B: Calculate intensive parenting time from the raw activity file.\n",
    "    part_b_df = calculate_intensive_parenting_time_1997(td_activity_df)\n",
    "    \n",
    "    # Combine the two parts into a single time-use dataset.\n",
    "    if part_a_df is not None:\n",
    "        time_use_df = pd.merge(part_a_df, part_b_df, on=['ID_1997', 'SN_1997'], how='outer')\n",
    "    else:\n",
    "        time_use_df = part_b_df\n",
    "\n",
    "    time_use_df.fillna(0, inplace=True)\n",
    "    print(\"1997 Time Diary processing complete.\")\n",
    "    return time_use_df\n",
    "\n",
    "def calculate_aggregate_weekly_hours_1997(base_df, td_agg_df):\n",
    "    \"\"\"Calculates weekly average hours for 39 broad activity categories for 1997.\"\"\"\n",
    "    td_agg_df.rename(columns={'AGGRID97': 'ID_1997', 'AGGRSN97': 'SN_1997'}, inplace=True)\n",
    "    panel_with_agg = pd.merge(base_df, td_agg_df, on=['ID_1997', 'SN_1997'], how='left')\n",
    "    activity_codes = [f'39{i:02d}' for i in range(1, 40)]\n",
    "    for code in activity_codes:\n",
    "        wd_col, we_col = f'WD97{code}', f'WE97{code}'\n",
    "        new_col = f'weekly_avg_hrs_cat_{code}_97'\n",
    "        if wd_col in panel_with_agg.columns and we_col in panel_with_agg.columns:\n",
    "            wd_sec = panel_with_agg[wd_col].fillna(0)\n",
    "            we_sec = panel_with_agg[we_col].fillna(0)\n",
    "            panel_with_agg[new_col] = ((wd_sec * 5) + (we_sec * 2)) / 3600\n",
    "    new_cols = ['ID_1997', 'SN_1997'] + [col for col in panel_with_agg.columns if col.startswith('weekly_avg_hrs_cat_')]\n",
    "    return panel_with_agg[new_cols]\n",
    "\n",
    "def calculate_intensive_parenting_time_1997(td_activity_df):\n",
    "    \"\"\"Processes the raw 1997 activity file to calculate 'intensive parenting' measures.\"\"\"\n",
    "    # Define activity codes associated with skill-building interactions.\n",
    "    skill_codes = [5490, 5491, 5492, 5493, 5494, 8010, 8011, 8012, 5040, 8020, 8030, 8040, 8090, 8510, 8520, 8211, 8212, 8213, 8214, 8215, 8221, 8222, 8223]\n",
    "    \n",
    "    # Column names from the raw TD97.csv file header.\n",
    "    activity_col = 'COLA'\n",
    "    day_of_week_col = 'T1'\n",
    "    duration_col = 'DURATION'\n",
    "    mother_col = 'COLG_B'\n",
    "    father_col = 'COLG_C'\n",
    "\n",
    "    skill_df = td_activity_df[td_activity_df[activity_col].isin(skill_codes)].copy()\n",
    "    \n",
    "    # Separate diaries into weekday (<=5) and weekend (>5).\n",
    "    wd_skill_df = skill_df[skill_df[day_of_week_col] <= 5]\n",
    "    we_skill_df = skill_df[skill_df[day_of_week_col] > 5]\n",
    "    \n",
    "    child_ids = td_activity_df[['TDID97', 'TDSN97']].drop_duplicates().rename(columns={'TDID97': 'ID_1997', 'TDSN97': 'SN_1997'})\n",
    "\n",
    "    # Calculate time spent with each parent for both diary types.\n",
    "    for day_type, df in [('wd', wd_skill_df), ('we', we_skill_df)]:\n",
    "        for parent, col in [('mother', mother_col), ('father', father_col)]:\n",
    "            mask = df[col] == 1\n",
    "            time = df[mask].groupby(['TDID97', 'TDSN97'])[duration_col].sum().reset_index()\n",
    "            time.rename(columns={duration_col: f'{parent}_interactive_{day_type}_sec_97', 'TDID97': 'ID_1997', 'TDSN97': 'SN_1997'}, inplace=True)\n",
    "            child_ids = pd.merge(child_ids, time, on=['ID_1997', 'SN_1997'], how='left')\n",
    "\n",
    "    # Ensure all columns exist and fill NaNs with 0.\n",
    "    cols_to_fill = [f'{p}_interactive_{d}_sec_97' for p in ['mother', 'father'] for d in ['wd', 'we']]\n",
    "    for col in cols_to_fill:\n",
    "        if col not in child_ids.columns: \n",
    "            child_ids[col] = 0\n",
    "        else: \n",
    "            child_ids[col] = child_ids[col].fillna(0)\n",
    "            \n",
    "    # Calculate total weekly hours from daily seconds, weighting for weekdays and weekends.\n",
    "    child_ids['parent_interactive_skill_hrs_wk_97'] = \\\n",
    "        (((child_ids.get('mother_interactive_wd_sec_97', 0) + child_ids.get('father_interactive_wd_sec_97', 0)) * 5) +\n",
    "         ((child_ids.get('mother_interactive_we_sec_97', 0) + child_ids.get('father_interactive_we_sec_97', 0)) * 2)) / 3600\n",
    "         \n",
    "    return child_ids[['ID_1997', 'SN_1997', 'parent_interactive_skill_hrs_wk_97']]\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # Create the analysis folder if it doesn't exist.\n",
    "    if not os.path.exists(ANALYSIS_PATH):\n",
    "        os.makedirs(ANALYSIS_PATH)\n",
    "\n",
    "    # Step 1: Merge core CDS data\n",
    "    core_cds_df = merge_core_cds_data_1997()\n",
    "    path_step1 = os.path.join(ANALYSIS_PATH, '01_cds_merged_1997.csv')\n",
    "    core_cds_df.to_csv(path_step1, index=False)\n",
    "    print(f\"Step 1 intermediate file saved to: {path_step1}\")\n",
    "\n",
    "    # Step 2: Merge PSID Family File\n",
    "    cds_family_df = merge_family_file_1997(core_cds_df)\n",
    "    path_step2 = os.path.join(ANALYSIS_PATH, '02_cds_with_family_data_1997.csv')\n",
    "    cds_family_df.to_csv(path_step2, index=False)\n",
    "    print(f\"Step 2 intermediate file saved to: {path_step2}\")\n",
    "\n",
    "    # Step 3: Merge longitudinal TAS data\n",
    "    if 'ER30001' in cds_family_df.columns:\n",
    "        cds_tas_panel = merge_longitudinal_tas_data(cds_family_df)\n",
    "        path_step3 = os.path.join(ANALYSIS_PATH, '03_cds_tas_panel_1997.csv')\n",
    "        cds_tas_panel.to_csv(path_step3, index=False)\n",
    "        print(f\"Step 3 intermediate file saved to: {path_step3}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Step 3: TAS Merge, because permanent IDs were not found.\")\n",
    "        cds_tas_panel = cds_family_df \n",
    "\n",
    "    # Step 4: Process Time Diary data\n",
    "    time_use_variables = process_time_diaries_1997()\n",
    "    path_step4 = os.path.join(ANALYSIS_PATH, '04_time_use_variables_1997.csv')\n",
    "    time_use_variables.to_csv(path_step4, index=False)\n",
    "    print(f\"Step 4 intermediate file saved to: {path_step4}\")\n",
    "\n",
    "    # Step 5: Final Merge\n",
    "    print(\"\\n--- Step 5: Final Merge ---\")\n",
    "    final_dataset = pd.merge(cds_tas_panel, time_use_variables, on=['ID_1997', 'SN_1997'], how='left')\n",
    "    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated()]\n",
    "    print(\"All 1997 data sources successfully merged.\")\n",
    "\n",
    "    # Save final and sample outputs\n",
    "    final_path = os.path.join(ANALYSIS_PATH, 'final_analysis_dataset_1997.csv')\n",
    "    final_dataset.to_csv(final_path, index=False)\n",
    "    print(f\"Final dataset saved to: {final_path}\")\n",
    "    \n",
    "    if len(final_dataset) >= 1000:\n",
    "        sample_df = final_dataset.sample(n=1000, random_state=42)\n",
    "        sample_path = os.path.join(ANALYSIS_PATH, 'sample_final_analysis_dataset_1997.csv')\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        print(f\"Sample dataset saved to: {sample_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
