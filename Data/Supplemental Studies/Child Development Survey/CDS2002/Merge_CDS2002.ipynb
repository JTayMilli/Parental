{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467d6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data files loaded and prepared successfully.\n",
      "Step 1: Base data established from DEMOG.csv. Shape: (2907, 22)\n",
      "Step 2: Generational Map data merged. Shape: (2907, 29)\n",
      "Step 3: All child-level data files merged. Shape: (2907, 1785)\n",
      "Step 4: ID Map data merged, adding PCG identifiers. Shape: (2907, 1791)\n",
      "Step 5: PCG Household data merged. Shape: (3411, 2007)\n",
      "Step 6: Merged dataset structure cleaned. Final Shape: (3411, 2007)\n",
      "\n",
      "--- Generating Clean Output Files ---\n",
      "Full clean merged dataset with 3411 rows and 2007 columns saved to 'full_merged_cds_data_clean.csv'\n",
      "Random sample of 1000 observations saved to 'sample_merged_cds_data_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "# merge_cds_data.py\n",
    "#\n",
    "# Purpose:\n",
    "# This script performs a sequential merge of multiple data files from the 2002 wave\n",
    "# of the Panel Study of Income Dynamics - Child Development Supplement (PSID-CDS).\n",
    "# The primary objective is to create a single, clean, child-level dataset that serves\n",
    "# as a foundational file for analyzing parenting dynamics. The final output is\n",
    "# structurally clean, with original variable names and values preserved, and includes\n",
    "# all necessary identifiers for future merges with other PSID waves or supplemental files.\n",
    "#\n",
    "# (Future prompts to modify this code will also be documented in this comprehensive manner)\n",
    "#\n",
    "# Author: Gemini\n",
    "# Date: September 23, 2025\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set a random seed to ensure that the random sample generated is reproducible.\n",
    "# Anyone running this script will get the exact same sample file.\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Loads all necessary CSV files from the '2002' subfolder.\n",
    "\n",
    "    This function is designed to be the single point of data ingestion. It uses a\n",
    "    dictionary, `file_info`, to manage filenames and to standardize the names of\n",
    "    the key identifier columns across different files. This standardization is\n",
    "    critical for ensuring the subsequent merge operations are straightforward and accurate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are short names for the dataframes (e.g., 'demog')\n",
    "              and values are the loaded pandas DataFrames. Returns None if files are missing.\n",
    "    \"\"\"\n",
    "    data_files = {}\n",
    "    folder_path = '2002'\n",
    "    \n",
    "    # This dictionary is the control center for the loading process.\n",
    "    # It maps a nickname for each dataset to:\n",
    "    #   1. The actual filename (e.g., 'DEMOG.csv').\n",
    "    #   2. A dictionary for renaming the key columns to a consistent standard.\n",
    "    #      For example, 'DEMID01' and 'CHLDID02' both refer to the 2001 wave Family ID,\n",
    "    #      so we rename them both to 'ID_2001' to use as a consistent merge key.\n",
    "    file_info = {\n",
    "        'demog': ('DEMOG.csv', {'DEMID01': 'ID_2001', 'DEMSN01': 'SN_2001'}),\n",
    "        'idmap': ('IDMAP02.csv', {'CHLDID02': 'ID_2001', 'CHLDSN02': 'SN_2001'}),\n",
    "        'gen_map': ('GEN_MAP.csv', {'CH_ID68': 'ER30001', 'CH_PN': 'ER30002', 'GENID01': 'ID_2001', 'GENSN01': 'SN_2001'}),\n",
    "        'pcg_chld': ('PCG_CHLD.csv', {'PCHID01': 'ID_2001', 'PCHSN01': 'SN_2001'}),\n",
    "        'child': ('CHILD.csv', {'CHLDID01': 'ID_2001', 'CHLDSN01': 'SN_2001'}),\n",
    "        'assessmt': ('ASSESSMT.csv', {'ASMTID01': 'ID_2001', 'ASMTSN01': 'SN_2001'}),\n",
    "        'ocg_chld': ('OCG_CHLD.csv', {'OCGCID01': 'ID_2001', 'OCGCSN01': 'SN_2001'}),\n",
    "        'pcg_hhld': ('PCG_HHLD.csv', {'PHHID01': 'PCGID02', 'PHHSN01': 'PCGSN02'}),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for key, (filename, rename_map) in file_info.items():\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            data_files[key] = pd.read_csv(full_path).rename(columns=rename_map)\n",
    "        \n",
    "        print(\"All data files loaded and prepared successfully.\")\n",
    "        return data_files\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Missing file - {e}. Please ensure all CSV files are in the '{folder_path}' directory.\")\n",
    "        return None\n",
    "\n",
    "def merge_data_clean(data_files):\n",
    "    \"\"\"\n",
    "    Performs a sequential merge of the prepared dataframes to create a single,\n",
    "    child-level dataset.\n",
    "\n",
    "    The merge strategy is crucial. We begin with a base file containing all children\n",
    "    and then layer on additional data. The 'how=\"left\"' parameter is used in all\n",
    "    merges to ensure that we keep every child from the original demographic file,\n",
    "    even if they don't have corresponding data in other files (e.g., younger children\n",
    "    without a child interview). This preserves the complete sample.\n",
    "\n",
    "    Args:\n",
    "        data_files (dict): A dictionary of the loaded and prepared DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A single, merged DataFrame.\n",
    "    \"\"\"\n",
    "    if data_files is None:\n",
    "        return None\n",
    "\n",
    "    # Step 1: Establish the Base DataFrame\n",
    "    # We start with the demographic file ('demog'). This file serves as the master list\n",
    "    # of all 2,907 children who participated in the 2002 CDS wave.\n",
    "    merged_df = data_files['demog']\n",
    "    print(f\"Step 1: Base data established from DEMOG.csv. Shape: {merged_df.shape}\")\n",
    "\n",
    "    # Step 2: Merge the Generational Map (GEN_MAP)\n",
    "    # This step is performed early to bring the permanent 1968 identifiers ('ER30001', 'ER30002')\n",
    "    # into the main dataframe. These are the most important long-term identifiers.\n",
    "    # The merge is performed on the 2001 wave IDs, which are common to both files.\n",
    "    merged_df = pd.merge(merged_df, data_files['gen_map'], on=['ID_2001', 'SN_2001'], how='left', suffixes=('', '_genmap'))\n",
    "    print(f\"Step 2: Generational Map data merged. Shape: {merged_df.shape}\")\n",
    "\n",
    "    # Step 3: Merge Child-Level Data Files\n",
    "    # These files contain information specific to each child (e.g., their own interview,\n",
    "    # assessments). They can be directly merged onto the base frame using the\n",
    "    # child's 2001 wave ID.\n",
    "    child_level_dfs = {'pcg_chld': data_files['pcg_chld'], 'child': data_files['child'],\n",
    "                       'assessmt': data_files['assessmt'], 'ocg_chld': data_files['ocg_chld']}\n",
    "    for name, df in child_level_dfs.items():\n",
    "        # The `suffixes` argument handles any columns with the same name in different files\n",
    "        # by appending a suffix (e.g., '_child'), preventing data loss.\n",
    "        merged_df = pd.merge(merged_df, df, on=['ID_2001', 'SN_2001'], how='left', suffixes=('', f'_{name}'))\n",
    "    print(f\"Step 3: All child-level data files merged. Shape: {merged_df.shape}\")\n",
    "\n",
    "    # Step 4: Merge the ID Map (IDMAP02)\n",
    "    # This map is a crucial bridge. It links each CHILD's 2001 ID to their\n",
    "    # PRIMARY CAREGIVER's (PCG) 2001 ID ('PCGID02', 'PCGSN02'). This allows us\n",
    "    # to subsequently link household-level data.\n",
    "    merged_df = pd.merge(merged_df, data_files['idmap'], on=['ID_2001', 'SN_2001'], how='left', suffixes=('', '_idmap'))\n",
    "    print(f\"Step 4: ID Map data merged, adding PCG identifiers. Shape: {merged_df.shape}\")\n",
    "\n",
    "    # Step 5: Merge the PCG Household Data\n",
    "    # Now that we have the PCG's 2001 ID for each child, we can merge the household data,\n",
    "    # which was collected from the PCG. This correctly assigns the same household\n",
    "    # information to all children living in that household.\n",
    "    merged_df = pd.merge(merged_df, data_files['pcg_hhld'], on=['PCGID02', 'PCGSN02'], how='left', suffixes=('', '_pcghhld'))\n",
    "    print(f\"Step 5: PCG Household data merged. Shape: {merged_df.shape}\")\n",
    "    \n",
    "    # Step 6: Final Structural Cleaning\n",
    "    # The merge process, especially with `suffixes`, can create duplicate columns if they\n",
    "    # existed in multiple files with the same name and values. This line removes them.\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    print(f\"Step 6: Merged dataset structure cleaned. Final Shape: {merged_df.shape}\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # This is the main execution block that runs when the script is executed.\n",
    "    \n",
    "    # First, load and prepare all the raw data files.\n",
    "    all_data = load_and_prepare_data()\n",
    "    \n",
    "    # Only proceed if the data loading was successful.\n",
    "    if all_data:\n",
    "        # Second, perform the clean merge of the prepared data.\n",
    "        final_df = merge_data_clean(all_data)\n",
    "\n",
    "        if final_df is not None:\n",
    "            # --- Create Output Files ---\n",
    "            print(\"\\n--- Generating Clean Output Files ---\")\n",
    "            \n",
    "            # Save the full, clean merged dataset. This is your primary analysis file.\n",
    "            full_output_filename = 'full_merged_cds_data_clean.csv'\n",
    "            final_df.to_csv(full_output_filename, index=False)\n",
    "            print(f\"Full clean merged dataset with {final_df.shape[0]} rows and {final_df.shape[1]} columns saved to '{full_output_filename}'\")\n",
    "\n",
    "            # Create and save a random sample of 1000 for exploratory work.\n",
    "            if len(final_df) >= 1000:\n",
    "                sample_df = final_df.sample(n=1000)\n",
    "                sample_output_filename = 'sample_merged_cds_data_clean.csv'\n",
    "                sample_df.to_csv(sample_output_filename, index=False)\n",
    "                print(f\"Random sample of 1000 observations saved to '{sample_output_filename}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa6db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Time Diary Files ---\n",
      "Loaded aggregate TD data (Shape: (2569, 1311))\n",
      "Loaded raw activity TD data (Shape: (99467, 34))\n",
      "\n",
      "--- Part A: Calculating Broad Weekly Time Use Averages ---\n",
      "Aggregate weekly average calculations complete.\n",
      "\n",
      "--- Part B: Calculating Intensive Parenting Measures ---\n",
      "Intensive parenting variable calculation complete.\n",
      "\n",
      "--- Standalone Time Use Dataset Complete ---\n",
      "Final dataset saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Analysis Files\\cds_2002_time_use_variables.csv\n",
      "Random sample of 1000 observations saved to: C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Analysis Files\\sample_cds_2002_time_use_variables.csv\n"
     ]
    }
   ],
   "source": [
    "# process_all_time_diaries.py\n",
    "#\n",
    "# Purpose:\n",
    "# This script serves as a standalone module for processing all 2002 Time Diary data.\n",
    "# It is designed to be independent of other merged files. It reads the raw\n",
    "# time diary files and produces a clean dataset containing only child identifiers\n",
    "# and a comprehensive set of constructed time-use variables.\n",
    "#\n",
    "# This output file can then be easily merged with the main longitudinal panel.\n",
    "#\n",
    "# The script integrates two analytical approaches:\n",
    "#   1. Aggregate Analysis: Calculates \"composite week\" average hours for 39 broad\n",
    "#      activity categories.\n",
    "#   2. Contextual Analysis: Creates nuanced measures of \"intensive parenting,\"\n",
    "#      inspired by Doepke and Zilibotti (2017), by quantifying \"interactive skill time.\"\n",
    "#\n",
    "# (Future prompts to modify this code will also be documented in this comprehensive manner)\n",
    "#\n",
    "# Author: Gemini\n",
    "# Date: September 23, 2025\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_comprehensive_time_use_variables():\n",
    "    \"\"\"\n",
    "    Main function to load the time diary files and orchestrate the creation of\n",
    "    both aggregate and contextual time use variables.\n",
    "    \"\"\"\n",
    "    # --- Configuration: Define file paths ---\n",
    "    base_path = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Supplemental Studies\\Child Development Survey\\CDS2002\\2002'\n",
    "    agg_td_path = os.path.join(base_path, 'TD02_ACT_AGG.csv')\n",
    "    activity_td_path = os.path.join(base_path, 'TD_ACTIVITY.csv')\n",
    "\n",
    "    try:\n",
    "        print(\"--- Loading Time Diary Files ---\")\n",
    "        td_agg_df = pd.read_csv(agg_td_path, low_memory=False)\n",
    "        td_activity_df = pd.read_csv(activity_td_path, low_memory=False)\n",
    "        print(f\"Loaded aggregate TD data (Shape: {td_agg_df.shape})\")\n",
    "        print(f\"Loaded raw activity TD data (Shape: {td_activity_df.shape})\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: A required input file was not found. {e}\")\n",
    "        return None\n",
    "\n",
    "    # Create a base DataFrame with just the unique child identifiers from the aggregate file.\n",
    "    # These identifiers will be used to link all newly created variables.\n",
    "    child_identifiers = td_agg_df[['AGGRID01', 'AGGRSN01']].copy()\n",
    "    child_identifiers.rename(columns={'AGGRID01': 'ID_2001', 'AGGRSN01': 'SN_2001'}, inplace=True)\n",
    "\n",
    "    # --- Part A: Calculate the 39 broad weekly averages ---\n",
    "    time_use_df_part_a = calculate_aggregate_weekly_hours(child_identifiers, td_agg_df)\n",
    "\n",
    "    # --- Part B: Calculate the intensive parenting measures ---\n",
    "    time_use_df_part_b = calculate_intensive_parenting_time(td_activity_df)\n",
    "    \n",
    "    # --- Part C: Combine all time use variables into one file ---\n",
    "    # Merge the intensive parenting measures onto the aggregate measures.\n",
    "    final_time_use_df = pd.merge(time_use_df_part_a, time_use_df_part_b, on=['ID_2001', 'SN_2001'], how='left')\n",
    "    \n",
    "    # Fill any NaNs that result from the merge (e.g., a child has aggregate data but no\n",
    "    # interactive skill time) with 0.\n",
    "    final_time_use_df.fillna(0, inplace=True)\n",
    "\n",
    "    return final_time_use_df\n",
    "\n",
    "def calculate_aggregate_weekly_hours(base_df, td_agg_df):\n",
    "    \"\"\"\n",
    "    Calculates weekly average hours for 39 broad activity categories.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Part A: Calculating Broad Weekly Time Use Averages ---\")\n",
    "    \n",
    "    td_agg_df.rename(columns={'AGGRID01': 'ID_2001', 'AGGRSN01': 'SN_2001'}, inplace=True)\n",
    "    \n",
    "    # Merge the aggregate data onto the base identifier frame.\n",
    "    panel_with_agg = pd.merge(base_df, td_agg_df, on=['ID_2001', 'SN_2001'], how='left')\n",
    "\n",
    "    activity_codes = [f'39{i:02d}' for i in range(1, 40)]\n",
    "    for code in activity_codes:\n",
    "        wd_col = f'WD02{code}'\n",
    "        we_col = f'WE02{code}'\n",
    "        new_col_name = f'weekly_avg_hrs_cat_{code}'\n",
    "\n",
    "        if wd_col in panel_with_agg.columns and we_col in panel_with_agg.columns:\n",
    "            wd_seconds = panel_with_agg[wd_col].fillna(0)\n",
    "            we_seconds = panel_with_agg[we_col].fillna(0)\n",
    "            total_weekly_seconds = (wd_seconds * 5) + (we_seconds * 2)\n",
    "            panel_with_agg[new_col_name] = total_weekly_seconds / 3600\n",
    "    \n",
    "    # Keep only the identifiers and the newly created weekly average columns.\n",
    "    new_cols = ['ID_2001', 'SN_2001'] + [f'weekly_avg_hrs_cat_{code}' for code in activity_codes]\n",
    "    result_df = panel_with_agg[new_cols]\n",
    "            \n",
    "    print(\"Aggregate weekly average calculations complete.\")\n",
    "    return result_df\n",
    "\n",
    "def calculate_intensive_parenting_time(td_activity_df):\n",
    "    \"\"\"\n",
    "    Processes the raw activity file to calculate measures of \"intensive parenting.\"\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Part B: Calculating Intensive Parenting Measures ---\")\n",
    "\n",
    "    skill_building_codes = [\n",
    "        5490, 5491, 5492, 5493, 5494, 8010, 8011, 8012, 5040, 8020, 8030,\n",
    "        8040, 8090, 8510, 8520, 8211, 8212, 8213, 8214, 8215, 8221, 8222, 8223\n",
    "    ]\n",
    "    \n",
    "    skill_activities_df = td_activity_df[td_activity_df['COLA_02'].isin(skill_building_codes)].copy()\n",
    "    wd_skill_df = skill_activities_df[skill_activities_df['DIARY_02'] == 0]\n",
    "    we_skill_df = skill_activities_df[skill_activities_df['DIARY_02'] == 1]\n",
    "    \n",
    "    # Base dataframe for this part is all unique children in the activity file\n",
    "    child_ids_in_activity = td_activity_df[['TDID01', 'TDSN01']].drop_duplicates()\n",
    "    child_ids_in_activity.rename(columns={'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'}, inplace=True)\n",
    "\n",
    "\n",
    "    for day_type, df in [('wd', wd_skill_df), ('we', we_skill_df)]:\n",
    "        # Mother\n",
    "        mother_mask = df['COLGB_02'] == 1\n",
    "        mother_time = df[mother_mask].groupby(['TDID01', 'TDSN01'])['DUR_02'].sum().reset_index()\n",
    "        mother_time.rename(columns={'DUR_02': f'mother_interactive_{day_type}_sec', 'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'}, inplace=True)\n",
    "        child_ids_in_activity = pd.merge(child_ids_in_activity, mother_time, on=['ID_2001', 'SN_2001'], how='left')\n",
    "        \n",
    "        # Father\n",
    "        father_mask = df['COLGC_02'] == 1\n",
    "        father_time = df[father_mask].groupby(['TDID01', 'TDSN01'])['DUR_02'].sum().reset_index()\n",
    "        father_time.rename(columns={'DUR_02': f'father_interactive_{day_type}_sec', 'TDID01': 'ID_2001', 'TDSN01': 'SN_2001'}, inplace=True)\n",
    "        child_ids_in_activity = pd.merge(child_ids_in_activity, father_time, on=['ID_2001', 'SN_2001'], how='left')\n",
    "\n",
    "    cols_to_fill = ['mother_interactive_wd_sec', 'mother_interactive_we_sec', 'father_interactive_wd_sec', 'father_interactive_we_sec']\n",
    "    for col in cols_to_fill:\n",
    "        if col not in child_ids_in_activity.columns:\n",
    "            child_ids_in_activity[col] = 0\n",
    "        else:\n",
    "            # Revised method to avoid the FutureWarning\n",
    "            child_ids_in_activity[col] = child_ids_in_activity[col].fillna(0)\n",
    "            \n",
    "    child_ids_in_activity['parent_interactive_skill_hrs_wk'] = \\\n",
    "        (((child_ids_in_activity['mother_interactive_wd_sec'] + child_ids_in_activity['father_interactive_wd_sec']) * 5) +\n",
    "         ((child_ids_in_activity['mother_interactive_we_sec'] + child_ids_in_activity['father_interactive_we_sec']) * 2)) / 3600\n",
    "\n",
    "    print(\"Intensive parenting variable calculation complete.\")\n",
    "    # Return only the identifiers and the final calculated variable\n",
    "    return child_ids_in_activity[['ID_2001', 'SN_2001', 'parent_interactive_skill_hrs_wk']]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_time_use_dataset = create_comprehensive_time_use_variables()\n",
    "\n",
    "    if final_time_use_dataset is not None:\n",
    "        output_dir = r'C:\\Users\\joshu\\Aussie\\Monash\\Parental\\Data\\Analysis Files'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        print(f\"\\n--- Standalone Time Use Dataset Complete ---\")\n",
    "        \n",
    "        output_filename = os.path.join(output_dir, 'cds_2002_time_use_variables.csv')\n",
    "        final_time_use_dataset.to_csv(output_filename, index=False)\n",
    "        print(f\"Final dataset saved to: {output_filename}\")\n",
    "\n",
    "        # Save a new random sample\n",
    "        if len(final_time_use_dataset) >= 1000:\n",
    "            sample_df = final_time_use_dataset.sample(n=1000)\n",
    "            sample_output_filename = os.path.join(output_dir, 'sample_cds_2002_time_use_variables.csv')\n",
    "            sample_df.to_csv(sample_output_filename, index=False)\n",
    "            print(f\"Random sample of 1000 observations saved to: {sample_output_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
